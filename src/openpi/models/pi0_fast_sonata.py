import dataclasses
import inspect
import logging
import typing
import jax
import jax.numpy as jnp
import einops
import numpy as np
import torch
from pathlib import Path
import math                    # ç”¨äºè®¡ç®— patch_size å¯¹é½

# ---- JAX <-> openpi å…¼å®¹ï¼šKeyArray åœ¨ JAX 0.4.14+ è¢«ç§»é™¤ ----
import jax.random as _jr
if not hasattr(_jr, "KeyArray"): _jr.KeyArray = jax.Array  # type: ignore[attr-defined]

from flax import nnx
# nnx_bridge allows bridging PyTorch modules to NNX (Flax) modules
import flax.nnx.bridge as nnx_bridge

from jax import ShapeDtypeStruct
from jax import pure_callback

from openpi.models import sonata_encoder  # This module should define the Sonata point cloud encoder
import openpi.models.gemma_fast as _gemma 
from openpi.models import pi0_fast as _pi0_fast
from openpi.models import siglip as _siglip
import openpi.models.model as _model  # for BaseModel and Observation
from openpi.shared import download     # utility for downloading resources like weights

# optional â€“ hugâ€‘hub ä¼˜å…ˆ
try:
    from huggingface_hub import hf_hub_download
    _HF_OK = True
except ImportError:          # ç¯å¢ƒé‡Œæ²¡è£… huggingface_hub ä¼šèµ°æ—§é€»è¾‘
    _HF_OK = False
logger = logging.getLogger("openpi")

# ç”¨äºç”Ÿæˆæµ‹è¯•ç”¨æ•°æ®çš„å·¥å…·å‡½æ•°
def _canonicalize_point_dict(pd):
    pd = {k: jnp.asarray(v) for k, v in pd.items()}  # â† ç”¨ jnp

    if pd["coord"].ndim == 3:      # (B,N,3) â†’ (B*N,3)
        B, N, _ = pd["coord"].shape
        pd["coord"] = jnp.reshape(pd["coord"], (B * N, 3))
        pd["feat"]  = jnp.reshape(pd["feat"],  (B * N, -1))
        pd["batch"] = jnp.repeat(jnp.arange(B, dtype=jnp.int32), N)
        pd["offset"] = jnp.cumsum(jnp.full((B,), N, dtype=jnp.int32))

    if "grid_size" in pd and pd["grid_size"].ndim == 3:
        pd["grid_size"] = jnp.reshape(pd["grid_size"], (-1, 3))

    for key in ("batch", "offset"):
        if key in pd:
            pd[key] = pd[key].astype(jnp.int32, copy=False)

    return pd

# -------- åœ¨æ—§ / æ–°ä¸¤ç±» Observation ä¹‹é—´ç»Ÿä¸€æŠ½å–ç‚¹äº‘ ----------
def _extract_point_batch(obs) -> tuple[dict[str, jnp.ndarray], jnp.ndarray] | None:
    """
    è¿”å› (pc_dict, batch_mask) æˆ– None

    æ”¯æŒä¸¤ç§æ¥æºï¼š
    1. æ—§ç‰ˆ :  obs.pointcloud_data
       - a) å·²æ˜¯ Sonata å…¼å®¹ dict      â†’ ç›´æ¥ä½¿ç”¨
       - b) [B, P, 6] ndarray          â†’ è‡ªåŠ¨å±•å¼€æˆ dict
    2. æ–°ç‰ˆ :  obs.point_clouds["pointcloud"]  +  obs.point_cloud_masks["pointcloud"]
    """
    # ---------- ğŸ’¡ æ–°æ¥å£ ----------
    if hasattr(obs, "point_clouds") and "pointcloud" in getattr(obs, "point_clouds"):
        pc_arr  = obs.point_clouds["pointcloud"]          # [B, M, 6]
        pc_mask = obs.point_cloud_masks["pointcloud"]     # [B]

        B, M, _ = pc_arr.shape
        coords  = pc_arr[..., :3].astype(jnp.float32)
        feats   = pc_arr[..., 3:].astype(jnp.float32)
        valid   = ~jnp.isnan(coords[..., 0])

        counts  = jnp.sum(valid, axis=1).astype(jnp.int32)         # [B]
        flat_id = valid.reshape(-1)
        flat_coord = coords.reshape(-1, 3)[flat_id]
        flat_feat  = feats.reshape(-1, feats.shape[-1])[flat_id]
        flat_batch = jnp.repeat(jnp.arange(B, dtype=jnp.int64), counts)
        offset     = jnp.cumsum(counts, dtype=jnp.int32)

        pc_dict = _canonicalize_point_dict(
            dict(coord=flat_coord, feat=flat_feat, batch=flat_batch, offset=offset)
        )

        # ğŸš© ç›´æ¥ä½¿ç”¨é™æ€ç»´åº¦ M ä½œä¸º pad é•¿åº¦ï¼Œé¿å… runâ€‘time int()
        max_len  = pc_arr.shape[1]                                 # == M
        mask     = einops.repeat(pc_mask, "b -> b l", l=max_len)
        return pc_dict, mask

    # ---------- ğŸ’¡ æ—§æ¥å£ ----------
    legacy = getattr(obs, "pointcloud_data", None)
    if legacy is None:
        return None

    # a) å·²ç»æ˜¯ dict
    if isinstance(legacy, dict) and "coord" in legacy:
        pc_dict = _canonicalize_point_dict(legacy)
    else:
        # b) assume [B, P, 6] array
        arr = jnp.asarray(legacy, dtype=jnp.float32)
        B, P, C = arr.shape
        coord = arr[..., :3].reshape(-1, 3)
        feat  = arr[..., 3:].reshape(-1, C - 3)
        batch = jnp.repeat(jnp.arange(B, dtype=jnp.int64), P)
        offset = jnp.cumsum(jnp.full((B,), P, dtype=jnp.int32))
        pc_dict = _canonicalize_point_dict(dict(coord=coord, feat=feat,
                                                batch=batch, offset=offset))

    # legacy è·¯å¾„å‡è®¾æ¯å¸§å›ºå®š P ä¸ªç‚¹
    B = pc_dict["offset"].shape[0]                 # é™æ€ batch_size
    P = pc_dict["coord"].shape[0] // B            # æ¯å¸§ç‚¹æ•°ï¼ˆé™æ€ï¼‰
    mask = jnp.ones((B, P), dtype=bool)
    return pc_dict, mask

# Alias the Sonata class from the sonata_encoder module for convenience
Sonata = sonata_encoder.Sonata

# ---------------------------------------------------------------------------
#  helper: è¿‡æ»¤æ‰ç›®æ ‡æ„é€ å‡½æ•°ä¸æ”¯æŒçš„ kwargs
# ---------------------------------------------------------------------------
def _filter_kwargs_for_call(target, kw_dict, *, verbose: bool = False):
    """
    Parameters
    ----------
    target : class | callable
        è¦å®ä¾‹åŒ– / è°ƒç”¨çš„å¯¹è±¡ï¼ˆå¦‚ _gemma.Moduleï¼‰
    kw_dict : Mapping[str, Any]
        åŸå§‹ kwargs
    verbose : bool
        æ˜¯å¦æ‰“å°è¢«ä¸¢å¼ƒçš„å­—æ®µ
    """
    sig = inspect.signature(target)
    accepted = {}
    dropped = []
    for k, v in kw_dict.items():
        if k in sig.parameters:
            accepted[k] = v
        else:
            dropped.append(k)
    if verbose and dropped:
        logger.debug("%s: dropped unused kwargs %s", target.__name__, dropped)
    return accepted

@dataclasses.dataclass(frozen=True)
class Pi0FASTSonataConfig(_pi0_fast.Pi0FASTConfig):
    """Configuration for the Pi0FASTSonata model (Pi0FAST with Sonata point cloud encoder)."""
    dtype: str = "bfloat16"
    paligemma_variant: _gemma.Variant = "gemma_2b"
    # Inherits default action_dim, action_horizon, max_token_len from Pi0FASTConfig (e.g., 32, 32, 250)
    use_pretrained_point: bool = True      # è°ƒè¯•é˜¶æ®µå¯è®¾ False è·³è¿‡ä¸‹è½½

    @property
    def model_type(self) -> _model.ModelType:
        # Reuse the PI0_FAST model type (since this is an extension of Pi0-FAST architecture)
        return _model.ModelType.PI0_FAST

    def create(self, rng: jax.Array) -> "Pi0FASTSonata":
        """Instantiate a Pi0FASTSonata model with random initialization."""
        return Pi0FASTSonata(self, rngs=nnx.Rngs(rng))

class Pi0FASTSonata(_model.BaseModel):
    """
    Pi0FASTSonata model: Extends Pi0-FAST to incorporate a Sonata point cloud encoder.
    Combines vision (SigLIP image encoder), language (PaLI-Gemma LLM), and point cloud (Sonata) encoders.
    """
    def __init__(self, config: Pi0FASTSonataConfig, rngs: nnx.Rngs):
        # Initialize base model (setup action_dim, action_horizon, etc.)
        super().__init__(config.action_dim, config.action_horizon, config.max_token_len)

        # --------------------------------------------------------------
        # è®¾å®šç»Ÿä¸€ deviceï¼ˆGPU å¦‚æœå¯ç”¨ï¼Œå¦åˆ™ CPUï¼‰
        # --------------------------------------------------------------
        self.device: torch.device = torch.device(
            "cuda" if torch.cuda.is_available() else "cpu"
        )
        
        # ------------------------------------------------------------------
        # 1) è¯­è¨€æ¨¡å‹  Gemma  ------------------------------------------------
        # ------------------------------------------------------------------
        # ------------------------------------------------------------------
        # Gemmaâ€‘fast LLM ----------------------------------------------------
        # ------------------------------------------------------------------
        pal_cfg = _gemma.get_config(config.paligemma_variant)

        # æ ¹æ® gemma_fast.Module çš„ __init__ ç­¾åè‡ªåŠ¨æ”¶é›†éœ€è¦çš„å‚æ•°
        gemma_sig = inspect.signature(_gemma.Module.__init__)
        pal_kwargs: dict[str, typing.Any] = {}
        for name in gemma_sig.parameters:
            if name == "self":          # è·³è¿‡ self
                continue
            if hasattr(pal_cfg, name):
                pal_kwargs[name] = getattr(pal_cfg, name)

        # è‹¥ get_config æ²¡è¿”å› variantï¼Œåˆ™ç”¨ä¼ å…¥çš„æšä¸¾å­—ç¬¦ä¸²è¡¥é½
        pal_kwargs.setdefault("variant", config.paligemma_variant)

        # å…¶å®ƒ dtype ç›¸å…³è¦†å†™
        pal_kwargs["embed_dtype"] = config.dtype
        pal_kwargs["cache_dtype"] = config.dtype

        llm = nnx_bridge.ToNNX(_gemma.Module(**pal_kwargs))

        # åˆå§‹åŒ–
        llm.lazy_init(rngs=rngs, method="init")

        # ------------------------------------------------------------------
        # 2) å›¾åƒç¼–ç å™¨  SigLIP  --------------------------------------------
        # ------------------------------------------------------------------
        _model_width = getattr(pal_cfg, "width", getattr(pal_cfg, "hidden_size", 1024))
        
        raw_img_kwargs = dict(
            # _siglip.Module å¯èƒ½ä¸éœ€è¦ num_classesï¼›å¦‚æœ signature é‡Œæ²¡æœ‰ä¼šè¢«è‡ªåŠ¨ä¸¢å¼ƒ
            num_classes=_model_width,
            variant="So400m/14",
            pool_type="none",
            scan=True,
            dtype_mm=config.dtype,
        )

        # img_kwargsåœ¨å®˜æ–¹å®ç°é‡Œä¼¼ä¹æ²¡æœ‰ï¼Œå› æ­¤æš‚æ—¶æ³¨é‡Šæ‰ï¼Œå¦‚æœåç»­å‘ç°æ²¡é—®é¢˜ï¼Œå¯ä»¥åˆ é™¤-----------
        #img_kwargs = _filter_kwargs_for_call(_siglip.Module, raw_img_kwargs, verbose=True)
        #img = nnx_bridge.ToNNX(_siglip.Module(**img_kwargs))
        # ---------------------------------------------------------------------------------

        img = nnx_bridge.ToNNX(_siglip.Module(**raw_img_kwargs))

        # Initialize image encoder with a dummy image to set dimensions
        dummy_image = next(iter(config.fake_obs(batch_size=1).images.values()))
        img.lazy_init(dummy_image, train=False, rngs=rngs)

        # ------------------------------------------------------------------
        # 3) åˆ›å»ºå¹¶åŠ è½½ç‚¹äº‘ç¼–ç å™¨ (Sonata) â€” å‚æ•°å¯¹é½ SpatialLMâ€‘1.1, åé¢å¯ä»¥æ”¹æˆconfigä¼ è¾“
        # ------------------------------------------------------------------
        enc_depths = (2, 2, 6, 2)        # 4 ä¸ª stageï¼Œåé¢è¦å¤šæ¬¡ç”¨åˆ°
        
        # ---------- Sonata hyperâ€‘params â€“ ä¸ ckpt ä¿æŒ 100â€¯% ä¸€è‡´ ----------
        sp_cfg = dict(
            in_channels   = 6,
            order         = ("z", "z-trans"),
            stride        = (2, 2, 2, 2),         # 5â€‘stage â‡’ 4 æ¬¡ä¸‹é‡‡æ ·
            enc_depths    = (3, 3, 3, 12, 3),
            enc_channels  = (48, 96, 192, 384, 512),   # â˜… æœ«ç«¯ 512
            enc_num_head  = (3, 6, 12, 24, 32),
            enc_patch_size= (1024,)*5,            # ckpt é»˜è®¤
            mlp_ratio     = 4.0,
            mask_token    = True,
            enc_mode      = "voxel",
            enable_fourier_encode = True,         # â˜… ckpt å« fourier+input_proj
            num_bins      = 1280,
        )
        point_model = Sonata(**sp_cfg)

        if config.use_pretrained_point:
            # ------------------------------------------------------------------
            # ä»…ä½¿ç”¨æœ¬åœ°ç²¾ç®€åçš„ SpatialLM1.1 Sonata æƒé‡
            # æ–‡ä»¶æ”¾ç½®: <repo_root>/openpi/pretrained/SpatialLM_Sonata_encoder.pth
            # å¦‚æœæ²¡æœ‰æ¨¡å‹ï¼Œè¯·ä½¿ç”¨uv run scripts/sonata_weight_gen.pyæ¥è·å–sonataçš„checkpoint
            # ------------------------------------------------------------------
            # è·¯å¾„æŒ‡å‘ <repo_root>/src/pretrain/
            ckpt_path = (
                Path(__file__).resolve().parents[2]  # -> â€¦/openpi/src
                / "pretrain" / "SpatialLM_Sonata_encoder.pth"
            )
            if not ckpt_path.is_file():
                raise FileNotFoundError(
                    f"Sonata é¢„è®­ç»ƒæƒé‡æœªæ‰¾åˆ°: {ckpt_path}\n"
                    "è¯·å…ˆè¿è¡Œ scripts/sonata_weight_gen.py ç”Ÿæˆæ–‡ä»¶ï¼Œ"
                    "å¹¶æ”¾ç½®åˆ° openpi/pretrained/ ç›®å½•ã€‚"
                )
            logger.info("Using local Sonata weights: %s", ckpt_path)


            # åŠ è½½æƒé‡ â€”â€” PyTorch 2.6+ é»˜è®¤ weights_only=Trueï¼Œä¼šæ‹¦ä½åŒ…å«
            # numpy æ ‡é‡çš„è€å¼ state_dictï¼Œè¿™é‡Œæ˜¾å¼å…³æ‰å³å¯ã€‚
            load_kwargs = dict(map_location="cpu")
            if "weights_only" in inspect.signature(torch.load).parameters:
                load_kwargs["weights_only"] = False

            raw_obj = torch.load(ckpt_path, **load_kwargs)

            # â‘ è‹¥æ˜¯è®­ç»ƒ checkpointï¼Œå…ˆå–å‡ºçœŸæ­£æƒé‡
            if isinstance(raw_obj, dict) and "state_dict" in raw_obj:
                state_dict = raw_obj["state_dict"]
            else:
                state_dict = raw_obj

            # â‘¡å»æ‰å¤šä½™å‰ç¼€ï¼ˆå¦‚ "module." æˆ– "model.")
            cleaned = {}
            for k, v in state_dict.items():
                if k.startswith(("module.", "model.", "student.backbone.", "student.")):
                    cleaned[k.split(".", 1)[1]] = v
                else:
                    cleaned[k] = v

            # â‘¢éƒ¨åˆ†åŒ¹é…å³å¯ï¼›strict=False ä¼šè·³è¿‡å¤šä½™é”®ï¼Œä¹Ÿä¼šæç¤ºå“ªäº›æ²¡åŠ è½½
            missing, unexpected = point_model.load_state_dict(
                cleaned, strict=False
            )
            if missing:
                logger.warning("Sonata weights: %d missing params, e.g. %s",
                               len(missing), missing[:5])
            if unexpected:
                logger.warning("Sonata weights: %d unexpected params, e.g. %s",
                               len(unexpected), unexpected[:5])
            logger.info("Loaded pretrained Sonata weights from %s", ckpt_path)

        else:
            logger.warning(
                "Pi0FASTâ€‘Sonata: use_pretrained_point=False -> "
                "Sonata encoder starts with random weights."
            )

        # -------- ä¿è¯ Sonata æ•´ä¸ªç½‘ç»œåœ¨ GPUï¼ˆå« spconv kernelï¼‰ --------
        point_model.to(self.device)
        point_model.eval()  # inference mode

        # ------------------------------------------------------------------
        # 4) å®šä¹‰ _TorchSonataWrapper â€”â€” ä¿æŒåŸ APIï¼Œæ–°å¢ GPU æ”¯æŒ
        # ------------------------------------------------------------------
        class _TorchSonataWrapper(torch.nn.Module):
            """
            - è¿½è¸ªæœŸï¼ˆJIT / lazy_initï¼‰åªè¿”å› ShapeDtypeStructï¼Œä¸è·‘çœŸæ¨¡å‹ã€‚
            - è¿è¡ŒæœŸé€šè¿‡ pure_callback æŠŠ numpy â†’ torch.cuda â†’ numpyã€‚
            """
            def __init__(self, pt_model: torch.nn.Module, device: torch.device):
                super().__init__()
                # ç¡®ä¿ pt_model å·²åœ¨ç›®æ ‡ device
                self.inner = pt_model.to(device).eval()
                self.device = device

            # ---------- å†…éƒ¨ util ----------
            @staticmethod
            @torch.no_grad()
            def _torch_forward(inner: torch.nn.Module,
                               host_dict: dict[str, np.ndarray],
                               device: torch.device
                               ) -> tuple[np.ndarray, np.int32]:      # â˜… è¿”å›äºŒå…ƒç»„
                """
                æ¥æ”¶ host ä¸Šçš„ numpy è¾“å…¥ â†’ torch.Tensor.cuda â†’ è¿è¡Œ â†’ numpy è¾“å‡º
                ç»Ÿä¸€è¿”å› float32 numpy æ•°ç»„
                """
                # ---------- å…ˆå®‰å…¨æ‰å¹³åŒ– ----------
                if host_dict["coord"].ndim != 2:         # (B,N,3) â†’ (B*N,3)
                    B, N, _ = host_dict["coord"].shape
                    host_dict["coord"] = host_dict["coord"].reshape(B * N, 3)
                    host_dict["feat"]  = host_dict["feat"].reshape(B * N, -1)
                    host_dict["batch"] = np.repeat(np.arange(B, dtype=np.int64), N)
                    host_dict["offset"] = np.cumsum(np.full((B,), N, dtype=np.int64))

                # grid_coord / grid_size å¯èƒ½æ¥è‡ªä½“ç´ ç½‘æ ¼ â†’ ä¿è¯éƒ½æ˜¯ (P,3)
                for _key in ("grid_coord", "grid_size"):
                    if _key in host_dict and host_dict[_key].ndim == 3:
                        host_dict[_key] = host_dict[_key].reshape(-1, 3)

                # æ— è®ºä¸Šæ¸¸å¦‚ä½•ï¼Œfeat å¿…é¡»ä¸º 2â€‘Dï¼›ä¸ SpatialLM å¯¹é½
                if host_dict["feat"].ndim != 2:
                    C = host_dict["feat"].shape[-1]
                    host_dict["feat"] = host_dict["feat"].reshape(-1, C)

                
                # ---------- è‹¥ç¼º offsetï¼Œåˆ™æ ¹æ® batch ç”Ÿæˆ ----------
                if "offset" not in host_dict:
                    if "batch" in host_dict:
                        # --------------------------------------------------
                        # 1. ä¿è¯ batch æ˜¯ 1â€‘D int64
                        # --------------------------------------------------
                        b_arr = np.asarray(host_dict["batch"]).astype(np.int64)
                        if b_arr.ndim > 1:
                            b_arr = b_arr.reshape(-1)
                        host_dict["batch"] = b_arr

                        # 2. ç»Ÿè®¡æ¯ä¸ª batch çš„ç‚¹æ•° â†’ å‰ç¼€å’Œ
                        counts = np.bincount(b_arr)
                        host_dict["offset"] = np.cumsum(counts, dtype=np.int64)
                    else:
                        # å• batch æƒ…å½¢
                        host_dict["offset"] = np.array(
                            [host_dict["coord"].shape[0]], dtype=np.int64
                        )

                assert host_dict["coord"].shape[0] == host_dict["offset"][-1], (
                    f"coord N={host_dict['coord'].shape[0]}, "
                    f"but offset[-1]={host_dict['offset'][-1]}"
                )

                # ---------- NaN æ›¿æ¢ ----------
                nan_mask = np.isnan(host_dict["coord"]).any(axis=1)
                if nan_mask.any():
                    # åæ ‡ / ç‰¹å¾ ç½® 0ï¼›batch / offset ä¿æŒ
                    host_dict["coord"][nan_mask] = 0.0
                    host_dict["feat"][nan_mask]  = 0.0

                # ---------- grid_coord ----------
                if "grid_coord" not in host_dict:
                    # voxel é‡åŒ–åˆ°æ•´æ•°ç½‘æ ¼
                    host_dict["grid_coord"] = np.round(
                        host_dict["coord"] / 0.01
                    ).astype(np.int32)

                # pure_callback æŠŠ jax.Array ç›´æ¥é€è¿‡æ¥ï¼›å¿…é¡»å…ˆè½¬æˆçœŸæ­£çš„ numpy
                tch_in = {
                    k: torch.from_numpy(np.asarray(v))  # â† å…³é”®ï¼šnp.asarray()
                    .to(device)
                    for k, v in host_dict.items()
                }
                # Sonata é‡Œè¦åš (batch << 48)ï¼Œå¿…é¡»æ˜¯ int64
                for key in ("batch", "offset"):
                    if key in tch_in:
                        tch_in[key] = tch_in[key].long()
                out = inner(tch_in)
                if isinstance(out, dict):                # å– "feat"
                    out = out.get("feat", list(out.values())[0])

                # -------- SpatialLMÂ å¼å›ºå®šé•¿åº¦è¡¥é›¶ --------
                patch_size = 1024                       # â† ä¸ enc_patch_size å¯¹é½
                real_len   = out.size(0)
                pad_to     = math.ceil(real_len / patch_size) * patch_size
                if real_len < pad_to:
                    pad = out.new_zeros(pad_to - real_len, out.size(1))
                    out = torch.cat([out, pad], dim=0)
                # ------------------------------------------

                return out.float().cpu().numpy(), np.int32(real_len)

            # ---------- forward ----------
            def forward(self, pc_dict, *, train: bool = False):
                host_inputs = {k: jnp.asarray(v) for k, v in pc_dict.items()}
                # tree_flatten è¿”å› (flat_list, treedef)ï¼›åè€…è´Ÿè´£åå‘å±•å¼€
                flat, treedef = jax.tree_util.tree_flatten(host_inputs)

                # ---------- é‡æ–°æ„é€  dummy_npï¼ˆé¿å… Tracerâ†’NumPyï¼‰ ----------
                dummy_np = {}
                for k, v in host_inputs.items():
                    shape, dtype = v.shape, v.dtype

                    if k == "grid_size":
                        # ç”¨ 128â€¯å¡«å……ï¼Œdtype å¿…é¡»ä¸åŸå§‹ä¸€è‡´ (int32)
                        dummy_np[k] = np.full(shape, 128, dtype=dtype)
                    elif k == "coord":
                        # â€”â€” è‹¥åŸ coord æ˜¯ (B,N,3) â†’ total = B*N â€”â€”
                        if v.ndim == 3:
                            B, N, _ = v.shape
                            total = B * N
                        else:
                            total = v.shape[0]
                        coords = np.arange(total * 3, dtype=dtype).reshape(total, 3)
                        dummy_np[k] = coords
                    elif k == "offset":
                        # æ ¹æ® **å±•å¹³åçš„ç‚¹æ•°** ç”Ÿæˆæ­£ç¡® offset
                        total = (
                            host_inputs["coord"].reshape(-1, 3).shape[0]
                            if host_inputs["coord"].ndim == 3
                            else host_inputs["coord"].shape[0]
                        )
                        dummy_np[k] = np.array([total], dtype=dtype)
                    elif k == "feat":
                        # ä¿è¯ dummy ç‰¹å¾ä¹Ÿæ˜¯ 2â€‘Dï¼Œä¸çœŸå®å‰å‘å½¢çŠ¶ä¸€è‡´
                        if v.ndim == 3:
                            B, N, C = v.shape
                            dummy_np[k] = np.zeros((B * N, C), dtype=dtype)
                        else:
                            dummy_np[k] = np.zeros(shape, dtype=dtype)
                    elif k in ("grid_coord", "grid_size"):
                        # å§‹ç»ˆå±•å¹³æˆ (P,3)
                        if v.ndim == 3:
                            dummy_np[k] = np.zeros(
                                (v.shape[0] * v.shape[1], 3), dtype=dtype
                            )
                        else:
                            dummy_np[k] = np.zeros(shape, dtype=dtype)
                    else:
                        dummy_np[k] = np.zeros(shape, dtype=dtype)
                # -----------------------------------------------------------

                dummy_feat, _ = self._torch_forward(self.inner, dummy_np, self.device)
                patch_size = dummy_feat.shape[0]                       # ==1024
                out_struct = (                                          # feat & valid_len
                    ShapeDtypeStruct(dummy_feat.shape, jnp.float32),
                    ShapeDtypeStruct((), jnp.int32),
                )

                def _host_call(*flat_np):
                    # flat_np æ˜¯å›ä¼ çš„æ‰å¹³åˆ—è¡¨/å…ƒç»„ï¼Œéœ€ç”¨ treedef.unflatten è¿˜åŸ
                    np_dict = treedef.unflatten(list(flat_np))
                    return self._torch_forward(self.inner, np_dict, self.device)

                feat, valid_len = pure_callback(_host_call, out_struct,
                                                *flat, vectorized=False)
                valid_mask = jnp.arange(patch_size, dtype=jnp.int32) < valid_len
                return feat, valid_mask                                  # â˜… tuple

            # ---------- NNX åˆå§‹åŒ– ----------
            def init_with_output(
                self,
                rngs,
                pc_dict,
                *,
                train: bool = False,
                method: typing.Any = None,
                **_,
            ):
                dummy_out = self.forward(pc_dict, train=train)
                return dummy_out, {}  # æœ¬ wrapper ä¸å«å¯è®­ç»ƒå‚æ•°

            # --------------------------------------------------------------
            # â˜… å…¼å®¹ nnxâ€‘bridge è°ƒç”¨ï¼šé‡è½½ applyï¼Œå¿½ç•¥ variables / rngs. ä»¥åéœ€è¦rngéœ€è¦å°†å…¶ä¼ å…¥ä½†ç›®å‰ä¼ å…¥ä¼šé€ æˆå…¼å®¹æ€§é—®é¢˜
            # --------------------------------------------------------------
            def apply(                       # type: ignore[override]
                self,
                _variables,                  # nnxâ€‘bridge ä¼ å…¥çš„å ä½å˜é‡åŒ…ï¼ˆunusedï¼‰
                *args,
                rngs=None,
                method: str | None = "forward",
                **kwargs,
            ):
                """
                â€¢ method ä¸º nnxâ€‘bridge æŒ‡å®šçš„å‡½æ•°åï¼Œä¾‹å¦‚ "forward"ã€
                  "init_with_output"ï¼›é»˜è®¤ = "forward"  
                â€¢ rngs ä»…åœ¨ lazy_init æ—¶ä¼šä¼ å…¥ï¼ŒSonata ä¸ä½¿ç”¨ï¼Œç›´æ¥ä¸¢å¼ƒ
                """
                if method is None:
                    method = "forward"
    
                # é€‰å®šè¢«è°ƒå‡½æ•°
                target_fn = getattr(self, method)
    
                # init_with_output çš„ç­¾åä¸º (rngs, pc_dict, â€¦)
                if method == "init_with_output":
                    return target_fn(rngs, *args, **kwargs)
    
                # å…¶ä½™æ–¹æ³•ï¼ˆforward ç­‰ï¼‰
                return target_fn(*args, **kwargs)
                
        # ------------------------------------------------------------------
        # 4â€‘bis) çº¿æ€§å±‚åŒ…è£…å™¨ï¼šè®©æ™®é€š Linear æ”¯æŒ lazy_init
        # ------------------------------------------------------------------
        class _TorchLinearWrapper(torch.nn.Module):
            def __init__(self, in_dim: int, out_dim: int, device: torch.device, *, bias: bool = True):
                super().__init__()
                self.inner = torch.nn.Linear(in_dim, out_dim, bias=bias).to(device)
                self.device = device

            # -------- hostâ€‘sideè®¡ç®— --------
            @staticmethod
            @torch.no_grad()
            def _torch_forward(inner: torch.nn.Linear,
                               mat_np: np.ndarray,
                               device: torch.device) -> np.ndarray:
                """
               çº¯ host è°ƒç”¨ï¼šnp -> torch(cuda) -> np
                """
                # ä¿è¯ä¸€å®šæ˜¯ NumPyï¼Œå†é€å…¥ PyTorch
                if not isinstance(mat_np, np.ndarray):
                    mat_np = np.asarray(mat_np)
                x = torch.from_numpy(mat_np).to(inner.weight.dtype).to(device)
                y = inner(x)
                return y.cpu().numpy()

            # -------- forwardï¼ˆJAX sideï¼‰--------
            def forward(self, x_jax: jax.Array, *, train: bool = False):
                """
                â€¢ åœ¨ jit å›¾é‡Œä»¥ pure_callback æ–¹å¼è°ƒç”¨ _torch_forward  
                â€¢ è¾“å‡ºä¿æŒ float32ï¼Œåç»­å† cast
                """
                # è®¡ç®—è¾“å‡ºå½¢çŠ¶ï¼ˆå®Œå…¨ç¬¦å·åŒ–ï¼Œä¸è§¦å‘ concreteï¼‰
                out_shape = (*x_jax.shape[:-1], self.inner.out_features)
                out_struct = ShapeDtypeStruct(out_shape, jnp.float32)

                def _host_call(mat):
                    # mat æ˜¯ numpy.ndarray ï¼ˆpure_callback å·²è½¬æ¢ï¼‰
                    return self._torch_forward(self.inner, mat, self.device)

                return pure_callback(_host_call, out_struct, x_jax, vectorized=False)

            # -------- lazy_init hook --------
            def init_with_output(self, rngs, x_np, *, method=None, **_):
                """
                lazy_initâ€¯é˜¶æ®µä¸ä¼šåœ¨ jit å†…ï¼Œ
                ç›´æ¥èµ° hostâ€‘side è®¡ç®—å³å¯ï¼ŒåŠ é€Ÿåˆå§‹åŒ–ã€‚
                """
                if isinstance(x_np, jax.Array):
                    x_np = np.asarray(jax.device_get(x_np))
                y = self._torch_forward(self.inner, x_np, self.device)
                return y, {}          # no trainable vars

            # -------- nnxâ€‘bridge å…¼å®¹ --------
            def apply(self, _vars, *args, rngs=None, method: str | None = "forward", **kw):
                if method is None:
                    method = "forward"
                fn = getattr(self, method)
                if method == "init_with_output":
                    return fn(rngs, *args, **kw)
                return fn(*args, **kw)


        # ---------- 5) wrap ä¸º NNX æ¨¡å—å¹¶ lazy_init ----------
        # â‘¤ çº¿æ€§æŠ•å½±ï¼š512 â†’ PaLIâ€‘Gemma hidden_size (2048)
        self.PointProjector = nnx_bridge.ToNNX(
            _TorchLinearWrapper(512, _model_width, self.device)
        )
        # small dummy â†’ lazy_init
        self.PointProjector.lazy_init(
            jnp.zeros((1, sp_cfg["enc_channels"][-1]), jnp.float32),
            rngs=rngs,
        )
        # è®©æŠ•å½±å±‚è·Ÿéš PyTorch çš„ deviceï¼Œä¸å¿…æ‰‹åŠ¨è¿ç§»é™¤éåç»­ç»§ç»­é‡å†™åˆ°jax

        N, B = 64, 1          # 64 points, 1 batch
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Sonata çº¦å®šçš„ Point ç»“æ„ï¼ˆåŠ¡å¿…æ‰å¹³åŒ–ï¼‰ï¼š
        #   coord : (Total_N, 3)   float32 / int32
        #   feat  : (Total_N, C)
        #   batch : (Total_N,)     **int64**  â† è¦èƒ½åš Â«batchÂ <<Â 48Â»
        #   offset: (B,)           ç´¯è®¡ç‚¹æ•°å‰ç¼€å’Œ  **int64**
        #   grid_size : (B, 3)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        raw_dummy_pc = {
            "coord": jnp.arange(N * 3, dtype=jnp.float32).reshape(N, 3),
            "feat":  jnp.zeros((N, 6), dtype=jnp.float32),
            "batch": jnp.zeros((N,),  dtype=jnp.int32),      # 1â€‘D
            "offset": jnp.array([N], dtype=jnp.int32),
            "grid_size": jnp.array([[128, 128, 128]], dtype=jnp.int32),
        }
        dummy_pc = _canonicalize_point_dict(raw_dummy_pc)

        point = nnx_bridge.ToNNX(_TorchSonataWrapper(point_model, self.device))
        # åˆå§‹åŒ– wrapperï¼ˆä¸€æ¬¡æ€§ shape æ¨æ–­ï¼‰
        point.lazy_init(dummy_pc, train=False, rngs=rngs)

        # ------------------------------------------------------------------
        # 6) æ‰“åŒ…æ‰€æœ‰å­æ¨¡å—
        # ------------------------------------------------------------------
        self.PaliGemma = nnx.Dict(
            llm   = llm,
            img   = img,
            point = point,
            point_proj = self.PointProjector,
        )

    def embed_inputs(
        self, obs: _model.Observation
    ) -> tuple[jax.Array, jax.Array, jax.Array]:
        """
        Embed all input modalities (images, point cloud, text tokens) into sequences of token embeddings.
        Returns:
            token_embeddings (jax.Array): concatenated token features of shape [B, N, emb_dim]
            input_mask (jax.Array[bool]): mask indicating valid tokens [B, N]
            ar_mask (jax.Array[int]): autoregressive mask for tokens [B, N] (0 where tokens attend freely, 1 where causal dependence begins).
        """
        token_embeddings = []
        input_mask = []
        ar_mask = []

        # 1. Image tokens (from image encoder)
        for cam_name, image in obs.images.items():
            img_tokens, _ = self.PaliGemma.img(image, train=False)  # shape [B, n_img_tokens, emb_dim]
            token_embeddings.append(img_tokens)
            # For each image token, the mask is valid if the image is present (broadcast image mask per token)
            mask = jnp.broadcast_to(obs.image_masks[cam_name][:, None], (obs.image_masks[cam_name].shape[0], img_tokens.shape[1]))
            input_mask.append(mask)
            # Image tokens have no autoregressive dependency amongst themselves (set AR mask = 0 for these tokens)
            ar_mask.append(jnp.zeros_like(mask, dtype=jnp.int32))

        # 2. Point cloud tokens --------------------------------------------------
        pc_pack = _extract_point_batch(obs)
        if pc_pack is not None:
            pc_dict, pt_mask = pc_pack            # pt_mask : [B, Lp]

            # --- è°ƒ Sonata ---
            pt_tokens, valid_mask = self.PaliGemma.point(pc_dict, train=False)   # [P, 512]  or [B,L,C] (æ—§æ¥å£)

            # --- çº¿æ€§æŠ•å½±åˆ° LLM dim ---
            pt_tokens = self.PointProjector(pt_tokens)
            if isinstance(pt_tokens, (tuple, list)):
                pt_tokens = pt_tokens[0]
            pt_tokens = pt_tokens.astype(token_embeddings[0].dtype)

            # è‹¥ä»æ˜¯æ‰å¹³ [P,C]ï¼Œæ ¹æ® offset è¿˜åŸ batchï¼Œå¹¶æŒ‰ patch_size=1024 è¡¥é½
            if pt_tokens.ndim == 2:
                splits = jnp.split(pt_tokens, pc_dict["offset"][:-1])
                patch_size = valid_mask.shape[0]              # == 1024
                pad = lambda x: jnp.pad(x,
                                        ((0, patch_size - x.shape[0]), (0, 0)))
                pt_tokens = jnp.stack([pad(s) for s in splits])       # [B,1024,C]

            # ------- åŒæ­¥æ‰©å±• pt_mask å¹¶ä¸ valid_mask ç»“åˆ -------
            pad_len = pt_tokens.shape[1] - pt_mask.shape[1]
            if pad_len > 0:        # åŸ mask å³ä¾§è¡¥ False ç›´åˆ° 1024
                pt_mask = jnp.pad(pt_mask,
                                  ((0, 0), (0, pad_len)),
                                  constant_values=False)

            pt_mask = pt_mask & valid_mask[None, :]   # åªä¿ç•™æœ‰æ•ˆ token

            token_embeddings.append(pt_tokens)
            input_mask.append(pt_mask)
            ar_mask.append(jnp.zeros_like(pt_mask, dtype=jnp.int32))

        # 3. Text tokens (from language model embedding)
        # Ensure textual inputs are present
        assert obs.tokenized_prompt is not None and obs.tokenized_prompt_mask is not None and obs.token_ar_mask is not None, \
            "Tokenized prompt and corresponding masks must be provided for text inputs."
        txt_tokens = self.PaliGemma.llm(obs.tokenized_prompt, embed_only=True)  # shape [B, L, emb_dim]
        token_embeddings.append(txt_tokens)
        input_mask.append(obs.tokenized_prompt_mask)
        ar_mask.append(obs.token_ar_mask)

        # Concatenate all modality token sequences along the sequence dimension
        tokens = jnp.concatenate(token_embeddings, axis=1)      # [B, N_total, emb_dim]
        mask = jnp.concatenate(input_mask, axis=1)              # [B, N_total]
        ar = jnp.concatenate(ar_mask, axis=1)                   # [B, N_total]
        return tokens, mask, ar

    def compute_loss(
        self,
        rng: jax.Array,
        observation: _model.Observation,
        actions: _model.Actions,
        *,
        train: bool = False
    ) -> jax.Array:
        """Compute sequence loss for the model (predict next token loss for language prompt)."""
        # Preprocess observation (normalize/augment images, ensure masks)
        # å‘å›¾åƒé¢„å¤„ç†å‡½æ•°ä¼ é€’ RNGï¼ˆä»…åœ¨è®­ç»ƒé˜¶æ®µéœ€è¦ï¼‰
        prep_rng = rng if train else None
        observation = _model.preprocess_observation(prep_rng, observation, train=train, image_keys=list(observation.images.keys()))
        # Embed all inputs to tokens and get masks
        tokens, mask, ar = self.embed_inputs(observation)
        # Compute attention mask for the sequence (prefix + causal masking as needed)
        attn_mask = _pi0_fast.make_attn_mask(mask, ar)
        # Prepare next-token prediction targets: one-hot encoding of tokenized_prompt shifted by one
        vocab_size = self.PaliGemma.llm.module.vocab_size
        targets = jax.nn.one_hot(observation.tokenized_prompt[:, 1:], vocab_size)  # shape [B, L-1, vocab_size]
        # Run LLM for prefix (all tokens except the last one, which will be predicted)
        pre_logits, _, _ = self.PaliGemma.llm(
            embedded_prefix=tokens[:, :-1],
            mask=attn_mask[:, :-1, :-1],
            return_prelogits=True
        )
        # Decode only the new suffix (the part to predict) to get logits for target tokens
        logits, _ = self.PaliGemma.llm(pre_logits=pre_logits[:, -targets.shape[1]:])
        log_probs = jax.nn.log_softmax(logits, axis=-1)
        # Compute cross-entropy loss on the target token predictions
        assert observation.token_loss_mask is not None, "Token loss mask must be provided for computing loss."
        loss_mask = observation.token_loss_mask[:, 1:]  # align with targets (skip the first token which has no previous token to predict it)
        token_nll = -jnp.sum(targets * log_probs, axis=-1)  # negative log-likelihood per token
        # Compute mean loss per sequence (only averaging over actual target tokens)
        seq_loss = jnp.sum(token_nll * loss_mask, axis=-1) / jnp.clip(jnp.sum(loss_mask, axis=-1), a_min=1)
        return seq_loss  # shape [B], per-sequence loss

    def sample_actions(
        self,
        rng: jax.Array,
        observation: _model.Observation,
        *,
        max_decoding_steps: int = 256,
        temperature: float = 0.0
    ) -> _model.Actions:
        """
        Autoregressively sample a sequence of actions (or action tokens) from the model given an observation.
        This uses the model as a prefix model (prefix = images + prompt tokens + optional point tokens),
        then generates additional tokens up to max_decoding_steps or until an EOS token is produced.
        """
        # Preprocess observation (no augmentation, just ensure correct shapes/masks)
        observation = _model.preprocess_observation(None, observation, train=False, image_keys=list(observation.images.keys()))
        # Embed inputs to get prefix token embeddings and masks
        prefix_tokens, prefix_mask, prefix_ar_mask = self.embed_inputs(observation)
        prefix_attn_mask = _pi0_fast.make_attn_mask(prefix_mask, prefix_ar_mask)
        # Align all prefix sequences to the right (required for caching in prefix)
        prefix_tokens, prefix_mask, prefix_attn_mask = _pi0_fast.left_to_right_align(prefix_tokens, prefix_mask, prefix_attn_mask)
        prefill_size = prefix_tokens.shape[1]  # total sequence length after alignment (prefix padded to some length)
        prefill_len = jnp.sum(prefix_mask, axis=-1)  # actual prefix length per batch (number of valid tokens)
        prefix_start = prefill_size - prefill_len   # start index of actual prefix tokens after right-align (for each batch)
        # Prepare attention mask for prefix + decoding steps
        prefix_attn_mask = jnp.pad(prefix_attn_mask, ((0, 0), (0, 0), (0, max_decoding_steps)))
        prefix_positions = jnp.cumsum(prefix_mask, axis=-1) - 1  # positions of prefix tokens (0-indexed)
        # Run the LLM in decoding mode to fill KV cache with prefix
        prefix_logits, kv_cache, _ = self.PaliGemma.llm(
            embedded_prefix=prefix_tokens,
            mask=prefix_attn_mask,
            positions=prefix_positions,
            decode=True
        )
        # Start from the last logit of the prefix as the beginning for new generation
        last_logit = prefix_logits[:, -1:]   # [B, 1, V]
        # Placeholder for generated token outputs (initialize with zeros)
        output_tokens = jnp.zeros((last_logit.shape[0], max_decoding_steps), dtype=jnp.int32)

        # Now run autoregressive decoding for at most max_decoding_steps
        tokens_to_decode = max_decoding_steps
        # Prepare attention mask for single-step decoding (prefix length + current step)
        # We will update the attention mask dynamically in the loop if needed

        # Use a while loop to generate tokens until done or max steps
        def cond_fn(state):
            _rng, _last, _cache, _step, _out = state
            has_eos = jax.lax.cond(
                _step == 0,
                lambda _: False,
                lambda _: jnp.all(_out[:, _step - 1] == _pi0_fast.PALIGEMMA_EOS_TOKEN),
                operand=None,
            )
            return jnp.logical_and(_step < max_decoding_steps, ~has_eos)

        def body_fn(state):
            rng_key, last_logits, cache, step, out_tokens = state
            rng_key, subkey = jax.random.split(rng_key)
            logits_step = last_logits.squeeze(1)
            token = jax.lax.cond(
                temperature > 1e-6,
                lambda key: jax.random.categorical(key, logits_step / temperature, axis=-1),
                lambda key: jnp.argmax(logits_step, axis=-1),
                operand=subkey,
            ).astype(jnp.int32)
            out_tokens = _pi0_fast.put_along_last_axis(out_tokens,
                                                       jnp.broadcast_to(step, (token.shape[0], 1)),
                                                       token[:, None])
            positions = prefix_start + step
            logits, cache = self.PaliGemma.llm(token=token[:, None],
                                               kv_cache=cache,
                                               positions=positions,
                                               decode=True)
            return rng_key, logits, cache, step+1, out_tokens

        init_state = (rng, last_logit, kv_cache, jnp.array(0, jnp.int32), output_tokens)
        _, _, _, final_step, output_seq = jax.lax.while_loop(cond_fn, body_fn, init_state)

        # Return the output sequence of tokens as the model's predicted "actions"
        # (In practice, these tokens might represent discretized actions or a planned sequence encoded as text tokens)
        return output_seq[:, :final_step]   # [B, <=max_dec_steps]