# è¯¥ç‰ˆæœ¬å·²çŸ¥é—®é¢˜ï¼ˆè¿™äº›é—®é¢˜ç›®å‰æš‚æ—¶ä¸ç”¨ç«‹åˆ»è§£å†³ï¼‰ï¼š
# è®­ç»ƒæ—¶æ¢¯åº¦	ä¸ä¼šå°è¯•å›ä¼ åˆ° Sonata/Projector	freezeâ€‘filter éåˆšéœ€, è¿™ä¸ªæˆ‘ä»¬åç»­å†è§£å†³ï¼Œå› ä¸ºæˆ‘ä»¬å®é™…ä¸Šè¦å…è®¸è®­ç»ƒï¼Œæ‰€ä»¥åè€Œä¸èƒ½å†»ç»“ï¼Œè¿™ä¸ªåé¢å†è¯´
# æ€§èƒ½æ½œåœ¨ç“¶é¢ˆ	CPUâ€¯â†”â€¯GPU copy / å¤šç¼–è¯‘	åç»­è¿­ä»£å¯èƒ½ä¼šå½±å“è¿™ä¸ªï¼Œæ‰€ä»¥é¦–å…ˆè§£å†³é—®é¢˜1
# 1024 token sizeï¼Œè¿™ä¸ªæš‚æ—¶ä¸èƒ½è®¾ç½®å¤ªå¤§å› ä¸ºä¼šç‚¸æ˜¾å­˜ï¼Œé¦–å…ˆä½¿ç”¨æç¤ºæ–¹å¼æ¥ç¡®å®šæ˜¯å¦ä¼šæœ‰è¿‡å¤šçš„æƒ…å†µï¼Œæ²¡æœ‰å°±ç»§ç»­è®­ç»ƒï¼Œæœ‰çš„è¯åç»­å†ç»§ç»­å¤„ç†
# æ³¨æ„ï¼Œgridä¼¼ä¹ä¸èƒ½æ˜¯è´Ÿæ•°ï¼

import dataclasses
import inspect
import logging
import typing
import jax
import jax.numpy as jnp
import einops
import numpy as np
import torch
from pathlib import Path

# ---- JAX <-> openpi å…¼å®¹ï¼šKeyArray åœ¨ JAX 0.4.14+ è¢«ç§»é™¤ ----
import jax.random as _jr
if not hasattr(_jr, "KeyArray"): _jr.KeyArray = jax.Array  # type: ignore[attr-defined]

from flax import nnx
# nnx_bridge allows bridging PyTorch modules to NNX (Flax) modules
import flax.nnx.bridge as nnx_bridge

from jax import ShapeDtypeStruct
from jax import pure_callback

from openpi.models import sonata_encoder  # This module should define the Sonata point cloud encoder
import openpi.models.gemma_fast as _gemma 
from openpi.models import pi0_fast as _pi0_fast
from openpi.models import siglip as _siglip
import openpi.models.model as _model  # for BaseModel and Observation
# from openpi.shared import download     # utility for downloading resources like weights, not used for now

# optional â€“ hugâ€‘hub ä¼˜å…ˆ
try:
    from huggingface_hub import hf_hub_download
    _HF_OK = True
except ImportError:          # ç¯å¢ƒé‡Œæ²¡è£… huggingface_hub ä¼šèµ°æ—§é€»è¾‘
    _HF_OK = False
logger = logging.getLogger("openpi")

# ç”¨äºç”Ÿæˆæµ‹è¯•ç”¨æ•°æ®çš„å·¥å…·å‡½æ•°
def _canonicalize_point_dict(pd):
    pd = {k: jnp.asarray(v) for k, v in pd.items()}  # â† ç”¨ jnp

    if pd["coord"].ndim == 3:      # (B,N,3) â†’ (B*N,3)
        B, N, _ = pd["coord"].shape
        pd["coord"] = jnp.reshape(pd["coord"], (B * N, 3))
        pd["feat"]  = jnp.reshape(pd["feat"],  (B * N, -1))
        # batch / offset å¿…é¡»ä¸º int64ï¼ŒSonata å†…éƒ¨è¦åš (batch << 48)
        pd["batch"]  = jnp.repeat(jnp.arange(B, dtype=jnp.int64), N)
        pd["offset"] = jnp.cumsum(jnp.full((B,), N, dtype=jnp.int64))

    if "grid_size" in pd and pd["grid_size"].ndim == 3:
        pd["grid_size"] = jnp.reshape(pd["grid_size"], (-1, 3))

    # ç»Ÿä¸€å¼ºåˆ¶ int64 ä»¥å…åç»­å† cast
    for key in ("batch", "offset"):
         if key in pd:
            pd[key] = pd[key].astype(jnp.int64, copy=False)

    # SpatialLM å…¨ç¨‹ä½¿ç”¨ int32 ä½“ç´ åæ ‡ï¼›å¼ºåˆ¶è½¬æ¢å¯æ¶ˆé™¤æ½œåœ¨æº¢å‡º
    if "grid_coord" in pd:
        # è¿™é‡Œæœ‰ä¸ªæ½œåœ¨çš„é—®é¢˜ï¼Œè‹¥ç”¨æˆ·è‡ªå·±ç»„è£…çš„grid_coordå·²ç»è¶…è¿‡65535ï¼Œåˆ™å¯èƒ½å†æ¬¡æº¢å‡ºï¼Œä½†è¿™ä¸ªå®é™…ä¸Šä¸å¤ªå¯èƒ½ï¼Œå› æ­¤åªå†™æ³¨é‡Šï¼Œç­‰ä»¥åå¦‚æœè®­ç»ƒæœ‰é—®é¢˜å†å›æ¥çœ‹ã€‚
        pd["grid_coord"] = pd["grid_coord"].astype(jnp.int32, copy=False)

    return pd

# -------- åœ¨æ—§ / æ–°ä¸¤ç±» Observation ä¹‹é—´ç»Ÿä¸€æŠ½å–ç‚¹äº‘ ----------
def _extract_point_batch(obs) -> tuple[dict[str, jnp.ndarray], jnp.ndarray] | None:
    """
    è¿”å› (pc_dict, batch_mask) æˆ– None

    æ”¯æŒä¸¤ç§æ¥æºï¼š
    1. æ—§ç‰ˆ :  obs.pointcloud_data
       - a) å·²æ˜¯ Sonata å…¼å®¹ dict      â†’ ç›´æ¥ä½¿ç”¨
       - b) [B, P, 6] ndarray          â†’ è‡ªåŠ¨å±•å¼€æˆ dict
    2. æ–°ç‰ˆ :  obs.point_clouds["pointcloud"]  +  obs.point_cloud_masks["pointcloud"]
    """
    # ---------- ğŸ’¡ æ–°æ¥å£ ----------
    if hasattr(obs, "point_clouds") and "pointcloud" in getattr(obs, "point_clouds"):
        pc_arr  = obs.point_clouds["pointcloud"]          # [B, M, 6]
        pc_mask = obs.point_cloud_masks["pointcloud"]     # [B]

        B, M, _ = pc_arr.shape
        # SpatialLMâ€‘Qwen çº¦å®š:
        #   0â€‘2 : ä½“ç´ ç½‘æ ¼åæ ‡ (int32)
        #   3â€‘5 : è¿ç»­ xyz (float32)
        #   6+ : å…¶ä»–è¯­ä¹‰ç‰¹å¾
        grid_int_raw = pc_arr[..., :3].astype(jnp.int32)       # (B,M,3)
        # --- å…³é”®ä¿®å¤ï¼šé€ batch å½’é›¶ï¼Œé˜² Morton ä½å®½æº¢å‡º -----------------
        grid_base   = grid_int_raw.min(axis=1, keepdims=True)  # (B,1,3)
        grid_int    = grid_int_raw - grid_base                 # ä¿è¯æ¯ç»´ â‰¥0
        coords   = pc_arr[..., 3:6].astype(jnp.float32)        # è¿ç»­ xyz
        feats    = pc_arr[..., 3:].astype(jnp.float32)         # xyz + è¯­ä¹‰

        # ä¸åœ¨ JAX ä¾§åšå¯å˜é•¿å±•å¹³ / NaN è¿‡æ»¤ï¼›ä¿æŒ (B,M,*) é™æ€å½¢çŠ¶ï¼Œ
        # æŠŠå±•å¹³ã€å» NaNã€é‡æ–°è®¡ç®— offset å®Œå…¨äº¤ç”±
        # _TorchSonataWrapper (hostâ€‘side) å¤„ç†ï¼Œ
        # ä»¥é¿å… jnp.repeat(counts) çš„ç¼–è¯‘æœŸåŠ¨æ€å¤§å°ã€‚
        pc_dict = _canonicalize_point_dict(
            dict(coord=coords, grid_coord=grid_int, feat=feats)
        )

        # ç›´æ¥ä½¿ç”¨é™æ€ç»´åº¦ M ä½œä¸º pad é•¿åº¦ï¼Œé¿å… runâ€‘time int()
        max_len  = pc_arr.shape[1]                                 # == M
        mask     = einops.repeat(pc_mask, "b -> b l", l=max_len)
        return pc_dict, mask

    # ---------- æ—§æ¥å£ ----------
    legacy = getattr(obs, "pointcloud_data", None)
    if legacy is None:
        return None

    # a) å·²ç»æ˜¯ dict
    if isinstance(legacy, dict) and "coord" in legacy:
        pc_dict = _canonicalize_point_dict(legacy)
    else:
        # b) assume legacy is a [B, P, 6] array  (xyz + feat)
        arr = jnp.asarray(legacy, dtype=jnp.float32)   # (B,P,6)
        B, P, C = arr.shape

        coord = arr[..., :3].reshape(-1, 3)            # (B*P,3)
        feat  = arr[..., 3:].reshape(-1, C - 3)        # (B*P,3)
        batch  = jnp.repeat(jnp.arange(B, dtype=jnp.int64), P)
        offset = jnp.cumsum(jnp.full((B,), P, dtype=jnp.int64))

        # ---------- grid_coord: å…ˆè½¬ int32ï¼Œå†é€ç»´å‡æœ€å°å€¼ ----------
        grid_raw   = coord.astype(jnp.int32)
        grid_coord = grid_raw - grid_raw.min(axis=0, keepdims=True)

        # æ‰“åŒ…åˆ° dict â†’ canonicalize
        pc_dict = _canonicalize_point_dict(
            dict(
                coord      = coord,        # è¿ç»­ xyz
                grid_coord = grid_coord,   # å½’é›¶åçš„ int32
                feat       = feat,
                batch      = batch,
                offset     = offset,
            )
        )

    # legacy è·¯å¾„å‡è®¾æ¯å¸§å›ºå®š P ä¸ªç‚¹
    B = pc_dict["offset"].shape[0]                 # é™æ€ batch_size
    P = pc_dict["coord"].shape[0] // B            # æ¯å¸§ç‚¹æ•°ï¼ˆé™æ€ï¼‰
    mask = jnp.ones((B, P), dtype=bool)
    return pc_dict, mask

# Alias the Sonata class from the sonata_encoder module for convenience
Sonata = sonata_encoder.Sonata

@dataclasses.dataclass(frozen=True)
class Pi0FASTSonataConfig(_pi0_fast.Pi0FASTConfig):
    """Configuration for the Pi0FASTSonata model (Pi0FAST with Sonata point cloud encoder)."""

    # å…³é”®å­—æ®µï¼šæ¯ä¸ªç‚¹çš„æ€»ç‰¹å¾ç»´åº¦ = 3(xyz) + N(extra feat)
    # å¯¹å½“å‰ DummyPointDatasetï¼š[coords/5,  rgb] â‡’ N=6 â‡’ 9
    point_feat_dim: int = 9

    dtype: str = "bfloat16"
    paligemma_variant: _gemma.Variant = "gemma_2b"
    # Inherits default action_dim, action_horizon, max_token_len from Pi0FASTConfig (e.g., 32, 32, 250)
    use_pretrained_point: bool = True      # è°ƒè¯•é˜¶æ®µå¯è®¾ False è·³è¿‡ä¸‹è½½

    @property
    def model_type(self) -> _model.ModelType:
        # Reuse the PI0_FAST model type (since this is an extension of Pi0-FAST architecture)
        return _model.ModelType.PI0_FAST

    def create(self, rng: jax.Array) -> "Pi0FASTSonata":
        """Instantiate a Pi0FASTSonata model with random initialization."""
        return Pi0FASTSonata(self, rngs=nnx.Rngs(rng))

class Pi0FASTSonata(_model.BaseModel):
    """
    Pi0FASTSonata model: Extends Pi0-FAST to incorporate a Sonata point cloud encoder.
    Combines vision (SigLIP image encoder), language (PaLI-Gemma LLM), and point cloud (Sonata) encoders.
    """
    def __init__(self, config: Pi0FASTSonataConfig, rngs: nnx.Rngs):
        # Initialize base model (setup action_dim, action_horizon, etc.)
        super().__init__(config.action_dim, config.action_horizon, config.max_token_len)

        # --------------------------------------------------------------
        # è®¾å®šç»Ÿä¸€ deviceï¼ˆGPU å¦‚æœå¯ç”¨ï¼Œå¦åˆ™ CPUï¼‰
        # --------------------------------------------------------------
        self.device: torch.device = torch.device(
            "cuda" if torch.cuda.is_available() else "cpu"
        )
        
        # ------------------------------------------------------------------
        # 1) è¯­è¨€æ¨¡å‹  Gemma  ------------------------------------------------
        # ------------------------------------------------------------------
        # ------------------------------------------------------------------
        # Gemmaâ€‘fast LLM ----------------------------------------------------
        # ------------------------------------------------------------------
        pal_cfg = _gemma.get_config(config.paligemma_variant)

        # æ ¹æ® gemma_fast.Module çš„ __init__ ç­¾åè‡ªåŠ¨æ”¶é›†éœ€è¦çš„å‚æ•°
        gemma_sig = inspect.signature(_gemma.Module.__init__)
        pal_kwargs: dict[str, typing.Any] = {}
        for name in gemma_sig.parameters:
            if name == "self":          # è·³è¿‡ self
                continue
            if hasattr(pal_cfg, name):
                pal_kwargs[name] = getattr(pal_cfg, name)

        # è‹¥ get_config æ²¡è¿”å› variantï¼Œåˆ™ç”¨ä¼ å…¥çš„æšä¸¾å­—ç¬¦ä¸²è¡¥é½
        pal_kwargs.setdefault("variant", config.paligemma_variant)

        # å…¶å®ƒ dtype ç›¸å…³è¦†å†™
        pal_kwargs["embed_dtype"] = config.dtype
        pal_kwargs["cache_dtype"] = config.dtype

        llm = nnx_bridge.ToNNX(_gemma.Module(**pal_kwargs))

        # åˆå§‹åŒ–
        llm.lazy_init(rngs=rngs, method="init")

        # ------------------------------------------------------------------
        # 2) å›¾åƒç¼–ç å™¨  SigLIP  --------------------------------------------
        # ------------------------------------------------------------------
        _model_width = getattr(pal_cfg, "width", getattr(pal_cfg, "hidden_size", 1024))
        
        raw_img_kwargs = dict(
            # _siglip.Module å¯èƒ½ä¸éœ€è¦ num_classesï¼›å¦‚æœ signature é‡Œæ²¡æœ‰ä¼šè¢«è‡ªåŠ¨ä¸¢å¼ƒ
            num_classes=_model_width,
            variant="So400m/14",
            pool_type="none",
            scan=True,
            dtype_mm=config.dtype,
        )

        # ---------------------------------------------------------------------------------

        img = nnx_bridge.ToNNX(_siglip.Module(**raw_img_kwargs))

        # Initialize image encoder with a dummy image to set dimensions
        dummy_image = next(iter(config.fake_obs(batch_size=1).images.values()))
        img.lazy_init(dummy_image, train=False, rngs=rngs)

        # ------------------------------------------------------------------
        # 3) åˆ›å»ºå¹¶åŠ è½½ç‚¹äº‘ç¼–ç å™¨ (Sonata) â€” å‚æ•°å¯¹é½ SpatialLMâ€‘1.1, åé¢å¯ä»¥æ”¹æˆconfigä¼ è¾“
        # ------------------------------------------------------------------
        
        # ---------- Sonata hyperâ€‘params â€“ ä¸ ckpt ä¿æŒ 100â€¯% ä¸€è‡´ ----------
        # in_channels æŒ‰é…ç½® point_feat_dim â€“ 3 åŠ¨æ€ç¡®å®šï¼›è‹¥é…ç½®ç¼ºå¤±ç«‹åˆ»æŠ¥é”™
        if not hasattr(config, "point_feat_dim"):
            raise ValueError(
                "Pi0FASTSonataConfig éœ€æ˜¾å¼æä¾› point_feat_dimï¼Œç”¨äºæ¨å¯¼ "
                "Sonata in_channels = point_feat_dim - 3"
            )
        # ---------------- é…ç½® â†’ in_channelsï¼Œå¹¶ä¿ç•™äº¤å‰æ ¡éªŒ ---------------
        _in_channels = int(config.point_feat_dim) - 3
        if _in_channels <= 0:
            raise ValueError(
                f"é…ç½® point_feat_dim={config.point_feat_dim} æ— æ•ˆï¼Œé¡» â‰¥â€¯4"
            )
        # è‹¥åç»­è¾“å…¥ç‚¹äº‘ç‰¹å¾ç»´ä¸é…ç½®ä¸ç¬¦ï¼Œå°†åœ¨ embed_inputs æ—©æœŸæŠ¥é”™

        sp_cfg = dict(
            in_channels   = _in_channels,
            order         = ("z", "z-trans"),
            stride        = (2, 2, 2, 2),         # 5â€‘stage â‡’ 4 æ¬¡ä¸‹é‡‡æ ·
            enc_depths    = (3, 3, 3, 12, 3),
            enc_channels  = (48, 96, 192, 384, 512),   # â˜… æœ«ç«¯ 512
            enc_num_head  = (3, 6, 12, 24, 32),
            enc_patch_size= (1024,)*5,            # ckpt é»˜è®¤
            mlp_ratio     = 4.0,
            mask_token    = True,
            enc_mode      = "voxel",
            enable_fourier_encode = True,         # â˜… ckpt å« fourier+input_proj
            num_bins      = 1280,
        )
        point_model = Sonata(**sp_cfg)
        # è®°å½•ä¾›åç»­æ–­è¨€ï¼Projector ä½¿ç”¨
        self._point_in_channels = _in_channels
        self._enc_out_dim       = sp_cfg["enc_channels"][-1]   # e.g. 512

        if config.use_pretrained_point:
            # ------------------------------------------------------------------
            # ä»…ä½¿ç”¨æœ¬åœ°ç²¾ç®€åçš„ SpatialLM1.1 Sonata æƒé‡
            # æ–‡ä»¶æ”¾ç½®: <repo_root>/openpi/pretrained/SpatialLM_Sonata_encoder.pth
            # å¦‚æœæ²¡æœ‰æ¨¡å‹ï¼Œè¯·ä½¿ç”¨uv run scripts/sonata_weight_gen.pyæ¥è·å–sonataçš„checkpoint
            # ------------------------------------------------------------------
            # è·¯å¾„æŒ‡å‘ <repo_root>/src/pretrain/
            ckpt_path = (
                Path(__file__).resolve().parents[2]  # -> â€¦/openpi/src
                / "pretrain" / "SpatialLM_Sonata_encoder.pth"
            )
            if not ckpt_path.is_file():
                raise FileNotFoundError(
                    f"Sonata é¢„è®­ç»ƒæƒé‡æœªæ‰¾åˆ°: {ckpt_path}\n"
                    "è¯·å…ˆè¿è¡Œ scripts/sonata_weight_gen.py ç”Ÿæˆæ–‡ä»¶ï¼Œ"
                    "å¹¶æ”¾ç½®åˆ° openpi/pretrained/ ç›®å½•ã€‚"
                )
            logger.info("Using local Sonata weights: %s", ckpt_path)


            # åŠ è½½æƒé‡ â€”â€” PyTorch 2.6+ é»˜è®¤ weights_only=Trueï¼Œä¼šæ‹¦ä½åŒ…å«
            # numpy æ ‡é‡çš„è€å¼ state_dictï¼Œè¿™é‡Œæ˜¾å¼å…³æ‰å³å¯ã€‚
            load_kwargs = dict(map_location="cpu")
            if "weights_only" in inspect.signature(torch.load).parameters:
                load_kwargs["weights_only"] = False

            raw_obj = torch.load(ckpt_path, **load_kwargs)

            # â‘ è‹¥æ˜¯è®­ç»ƒ checkpointï¼Œå…ˆå–å‡ºçœŸæ­£æƒé‡
            if isinstance(raw_obj, dict) and "state_dict" in raw_obj:
                state_dict = raw_obj["state_dict"]
            else:
                state_dict = raw_obj

            # â‘¡å»æ‰å¤šä½™å‰ç¼€ï¼ˆå¦‚ "module." æˆ– "model.")
            cleaned = {}
            for k, v in state_dict.items():
                if k.startswith(("module.", "model.", "student.backbone.", "student.")):
                    cleaned[k.split(".", 1)[1]] = v
                else:
                    cleaned[k] = v

            # â‘¢éƒ¨åˆ†åŒ¹é…å³å¯ï¼›strict=False ä¼šè·³è¿‡å¤šä½™é”®ï¼Œä¹Ÿä¼šæç¤ºå“ªäº›æ²¡åŠ è½½
            missing, unexpected = point_model.load_state_dict(
                cleaned, strict=False
            )
            if missing:
                logger.warning("Sonata weights: %d missing params, e.g. %s",
                               len(missing), missing[:5])
            if unexpected:
                logger.warning("Sonata weights: %d unexpected params, e.g. %s",
                               len(unexpected), unexpected[:5])
            logger.info("Loaded pretrained Sonata weights from %s", ckpt_path)

        else:
            logger.warning(
                "Pi0FASTâ€‘Sonata: use_pretrained_point=False -> "
                "Sonata encoder starts with random weights."
            )

        # -------- ä¿è¯ Sonata æ•´ä¸ªç½‘ç»œåœ¨ GPUï¼ˆå« spconv kernelï¼‰ --------
        point_model.to(self.device)
        point_model.eval()  # inference mode

        # ------------------------------------------------------------------
        # 4) å®šä¹‰ _TorchSonataWrapper â€”â€” ä¿æŒåŸ APIï¼Œæ–°å¢ GPU æ”¯æŒ
        # ------------------------------------------------------------------
        class _TorchSonataWrapper(torch.nn.Module):
            """
            - è¿½è¸ªæœŸï¼ˆJIT / lazy_initï¼‰åªè¿”å› ShapeDtypeStructï¼Œä¸è·‘çœŸæ¨¡å‹ã€‚
            - è¿è¡ŒæœŸé€šè¿‡ pure_callback æŠŠ numpy â†’ torch.cuda â†’ numpyã€‚
            """
            def __init__(
                self,
                pt_model: torch.nn.Module,
                device: torch.device,
                patch_size: int,
                enc_out_dim: int,
            ):
                super().__init__()
                # ç¡®ä¿ pt_model å·²åœ¨ç›®æ ‡ device
                self.inner = pt_model.to(device).eval()
                self.device = device
                self.patch_size = patch_size
                self.enc_out_dim = enc_out_dim

            # ---------- å†…éƒ¨ util ----------
            @staticmethod
            @torch.no_grad()
            def _torch_forward(
                inner: torch.nn.Module,
                host_dict: dict[str, np.ndarray],
                device: torch.device,
                patch_size: int,
            ) -> tuple[np.ndarray, np.ndarray]:  # ç¬¬ 2 ä¸ª ndarray æ˜¯ bool æ©ç 
                """
                æ¥æ”¶ host ä¸Šçš„ numpy è¾“å…¥ â†’ torch.Tensor.cuda â†’ è¿è¡Œ â†’ numpy è¾“å‡º
                ç»Ÿä¸€è¿”å› float32 numpy æ•°ç»„
                """

                # =========================================================
                # â˜… æ–°å¢ï¼šè‹¥ä¸Šæ¸¸ä¼ å…¥ selected_batchï¼Œåªä¿ç•™è¿™ä¸€å¸§çš„ç‚¹
                #   (è¿™æ · embed_inputs é‡Œä¸ç”¨ v[sel]ï¼Œé¿å… JAX å¸ƒå°”åˆ‡ç‰‡æŠ¥é”™)
                # ---------------------------------------------------------
                if "selected_batch" in host_dict:
                    sb = int(host_dict.pop("selected_batch"))
                    sel = host_dict["batch"] == sb
                    for k, v in list(host_dict.items()):
                        # ä»…å¯¹ç¬¬ 0 ç»´ä¸ç‚¹æ•°å¯¹åº”çš„å­—æ®µåšç­›é€‰
                        if v.ndim and v.shape[0] == sel.shape[0]:
                            host_dict[k] = v[sel]
                    # å•æ ·æœ¬è¯­ä¹‰ï¼šbatch å…¨ 0ï¼Œoffset = [ç‚¹æ•°]
                    n_pts = host_dict["coord"].shape[0]
                    host_dict["batch"]  = np.zeros(n_pts, dtype=np.int64)
                    host_dict["offset"] = np.array([n_pts], dtype=np.int64)
                # =========================================================

                # ---------- å…ˆå®‰å…¨æ‰å¹³åŒ– ----------
                if host_dict["coord"].ndim != 2:         # (B,N,3) â†’ (B*N,3)
                    B, N, _ = host_dict["coord"].shape
                    host_dict["coord"] = host_dict["coord"].reshape(B * N, 3)
                    host_dict["feat"]  = host_dict["feat"].reshape(B * N, -1)
                    host_dict["batch"] = np.repeat(np.arange(B, dtype=np.int64), N)
                    host_dict["offset"] = np.cumsum(np.full((B,), N, dtype=np.int64))

                # grid_coord / grid_size å¯èƒ½æ¥è‡ªä½“ç´ ç½‘æ ¼ â†’ ä¿è¯éƒ½æ˜¯ (P,3)
                for _key in ("grid_coord", "grid_size"):
                    if _key in host_dict and host_dict[_key].ndim == 3:
                        host_dict[_key] = host_dict[_key].reshape(-1, 3)

                # æ— è®ºä¸Šæ¸¸å¦‚ä½•ï¼Œfeat å¿…é¡»ä¸º 2â€‘Dï¼›ä¸ SpatialLM å¯¹é½
                if host_dict["feat"].ndim != 2:
                    C = host_dict["feat"].shape[-1]
                    host_dict["feat"] = host_dict["feat"].reshape(-1, C)

                
                # ---------- è‹¥ç¼º offsetï¼Œåˆ™æ ¹æ® batch ç”Ÿæˆ ----------
                if "offset" not in host_dict:
                    if "batch" in host_dict:
                        # --------------------------------------------------
                        # 1. ä¿è¯ batch æ˜¯ 1â€‘D int64
                        # --------------------------------------------------
                        b_arr = np.asarray(host_dict["batch"]).astype(np.int64)
                        if b_arr.ndim > 1:
                            b_arr = b_arr.reshape(-1)
                        host_dict["batch"] = b_arr

                        # 2. ç»Ÿè®¡æ¯ä¸ª batch çš„ç‚¹æ•° â†’ å‰ç¼€å’Œ
                        counts = np.bincount(b_arr)
                        host_dict["offset"] = np.cumsum(counts, dtype=np.int64)
                    else:
                        # å• batch æƒ…å½¢
                        host_dict["offset"] = np.array(
                            [host_dict["coord"].shape[0]], dtype=np.int64
                        )

                assert host_dict["coord"].shape[0] == host_dict["offset"][-1], (
                    f"coord N={host_dict['coord'].shape[0]}, "
                    f"but offset[-1]={host_dict['offset'][-1]}"
                )

                # ---------- NaN è¿‡æ»¤ï¼šä¸ SpatialLMâ€‘Qwen å®Œå…¨ä¸€è‡´ ----------
                nan_mask = (
                    np.isnan(host_dict["coord"]).any(axis=1)
                    | np.isnan(host_dict["feat"]).any(axis=1)
                )
                if nan_mask.any():
                    keep = ~nan_mask
                    for k in ("coord", "feat", "grid_coord"):
                        if k in host_dict:
                            host_dict[k] = host_dict[k][keep]
                    host_dict["batch"] = host_dict["batch"][keep]
                    # é‡æ–°è®¡ç®— offset  (prefixâ€sum of perâ€‘batch counts)
                    counts = np.bincount(host_dict["batch"])
                    host_dict["offset"] = np.cumsum(counts, dtype=np.int64)
                
                # ===== SpatialLMâ€‘style ç©ºç‚¹äº‘ä¿æŠ¤ =====
                if host_dict["coord"].shape[0] == 0:
                    # â‘  è®¡ç®—è¾“å‡ºç»´åº¦ C_outï¼ˆä¸ Sonata encoder æœ«å±‚ä¸€è‡´ï¼‰
                    C_out = (
                        inner.input_proj.out_features
                        if getattr(inner, "enable_fourier_encode", False)
                        else inner.enc_channels[-1]
                    )
                    # â‘¡ è¿”å›å…¨é›¶ç‰¹å¾ + å…¨ False æœ‰æ•ˆæ ‡è®°
                    pad_feat   = np.zeros((patch_size, C_out), dtype=np.float32)
                    valid_mask = np.zeros((patch_size,),       dtype=bool)
                    return pad_feat, valid_mask

                # ---------- è‹¥ç¼º grid_coordï¼Œåˆ™ç”¨ floor(coord) å¹¶å½’é›¶ ----------
                if "grid_coord" not in host_dict:
                    gc = np.floor(host_dict["coord"]).astype(np.int32, copy=False)
                    gc -= gc.min(axis=0, keepdims=True)        # ä¿è¯æ¯ç»´ä» 0 å¼€å§‹
                    host_dict["grid_coord"] = gc

                # pure_callback æŠŠ jax.Array ç›´æ¥é€è¿‡æ¥ï¼›å¿…é¡»å…ˆè½¬æˆçœŸæ­£çš„ numpy
                tch_in = {
                    k: torch.from_numpy(np.asarray(v))  # â† å…³é”®ï¼šnp.asarray()
                    .to(device)
                    for k, v in host_dict.items()
                }
                # Sonata é‡Œè¦åš (batch << 48)ï¼Œå¿…é¡»æ˜¯ int64
                for key in ("batch", "offset"):
                    if key in tch_in:
                        tch_in[key] = tch_in[key].long()
                out = inner(tch_in)
                if isinstance(out, dict):                # å– "feat"
                    out = out.get("feat", list(out.values())[0])

                real_len = out.size(0)
                # SpatialLM: ä¸æˆªæ–­ï¼›å³ pad åˆ° patch_size çš„å€æ•°
                # --- ä¿ç•™å›ºå®š 1024â€‘padding ä»¥ç»´æŒé™æ€ shape (JAX éœ€æ±‚ï¼Œæ”¹æˆä¸å›ºå®šçš„ä»£ä»·å¾ˆå¤§ï¼Œå¦‚æœå•çº¯åŠ å¤§åˆ™å®¹æ˜“ç‚¸æ˜¾å­˜) ---
                # ------------------------------------------------------------------
                # æˆªæ–­å‰åšæ˜¾å¼æŠ¥è­¦ï¼šè¿™ä¸ªæ˜¯å› ä¸ºå¦‚æœæˆ‘ä»¬è¦æ”¹æˆéå›ºå®šï¼Œä¼šå¯¼è‡´æ— æ³•jaxåŒ–ï¼Œè®¡ç®—ä»£ä»·å¾ˆå¤§ï¼Œæ‰€ä»¥æˆ‘ä»¬ç”¨è¿™ç§æ–¹å¼è¿›è¡Œæµ‹è¯•ï¼Œæ¥çœ‹çœ‹æ•°æ®é›†èƒ½ä¸èƒ½æä¾›åˆç†æ•°æ®
                # ------------------------------------------------------------------
                MAX_TOKEN = patch_size                  # =1024 (enc_patch_size[-1])
                if real_len > MAX_TOKEN:
                    # ç›´æ¥é˜»æ–­ï¼šç”¨æˆ·å¿…é¡»æ˜¾å¼æé«˜ enc_patch_size[-1] æˆ–å‡å°‘ç‚¹æ•°
                    raise RuntimeError(
                        f"[Sonata] token_len={real_len} exceeds configured MAX_TOKEN "
                        f"({MAX_TOKEN}). Increase `enc_patch_size[-1]` or reduce the "
                        "number of points per sample."
                    )
                pad_len = MAX_TOKEN - real_len
                if pad_len:
                    pad = out.new_zeros(pad_len, out.size(1))
                    out = torch.cat([out, pad], 0)
                valid_mask = np.arange(MAX_TOKEN) < real_len
                return out.float().cpu().numpy(), valid_mask

            # ---------- forward ----------
            def forward(self, pc_dict, *, train: bool = False):
                host_inputs = {k: jnp.asarray(v) for k, v in pc_dict.items()}
                # tree_flatten è¿”å› (flat_list, treedef)ï¼›åè€…è´Ÿè´£åå‘å±•å¼€
                flat, treedef = jax.tree_util.tree_flatten(host_inputs)

                # ---------- é‡æ–°æ„é€  dummy_npï¼ˆé¿å… Tracerâ†’NumPyï¼‰ ----------
                dummy_np = {}
                for k, v in host_inputs.items():
                    shape, dtype = v.shape, v.dtype

                    if k == "grid_size":
                        # ç”¨ 128â€¯å¡«å……ï¼Œdtype å¿…é¡»ä¸åŸå§‹ä¸€è‡´ (int32)
                        dummy_np[k] = np.full(shape, 128, dtype=dtype)
                    elif k == "coord":
                        # â€”â€” è‹¥åŸ coord æ˜¯ (B,N,3) â†’ total = B*N â€”â€”
                        if v.ndim == 3:
                            B, N, _ = v.shape
                            total = B * N
                        else:
                            total = v.shape[0]
                        coords = np.arange(total * 3, dtype=dtype).reshape(total, 3)
                        dummy_np[k] = coords
                    elif k == "offset":
                        # æ ¹æ® **å±•å¹³åçš„ç‚¹æ•°** ç”Ÿæˆæ­£ç¡® offset
                        total = (
                            host_inputs["coord"].reshape(-1, 3).shape[0]
                            if host_inputs["coord"].ndim == 3
                            else host_inputs["coord"].shape[0]
                        )
                        dummy_np[k] = np.array([total], dtype=dtype)
                    elif k == "feat":
                        # ä¿è¯ dummy ç‰¹å¾ä¹Ÿæ˜¯ 2â€‘Dï¼Œä¸çœŸå®å‰å‘å½¢çŠ¶ä¸€è‡´
                        if v.ndim == 3:
                            B, N, C = v.shape
                            dummy_np[k] = np.zeros((B * N, C), dtype=dtype)
                        else:
                            dummy_np[k] = np.zeros(shape, dtype=dtype)
                    elif k in ("grid_coord", "grid_size"):
                        # å§‹ç»ˆå±•å¹³æˆ (P,3)
                        if v.ndim == 3:
                            dummy_np[k] = np.zeros(
                                (v.shape[0] * v.shape[1], 3), dtype=dtype
                            )
                        else:
                            dummy_np[k] = np.zeros(shape, dtype=dtype)
                    else:
                        dummy_np[k] = np.zeros(shape, dtype=dtype)
                # -----------------------------------------------------------

                dummy_feat, _ = self._torch_forward(
                    self.inner, dummy_np, self.device, self.patch_size
                )
                # ä¾‹å¦‚é™æ€ä¸Šé™, è¿™é‡Œè®¾ç½®1024ï¼Œå¯ä»¥è®¾ç½®8192 (= 8 patch)ï¼›ç¡®ä¿ä¸ä¼šåœ¨çœŸå®è¾“å…¥ä¸­è¶…å‡ºä½†æ˜¯è¿™æ ·ä¼šç‚¸æ˜¾å­˜,æ‰€ä»¥æˆ‘ä»¬ä¿ç•™1024å®ç°, åç»­å¯ä»¥ç»§ç»­åŠ 
                # patch_size ç”± Wrapper æ„é€ å‡½æ•°ä¼ å…¥ï¼›ä¿æŒä¸ Sonata enc_patch_size ä¸€è‡´
                MAX_TOKEN = self.patch_size
                C = dummy_feat.shape[1]
                out_struct = (ShapeDtypeStruct((MAX_TOKEN, C), jnp.float32),
                              ShapeDtypeStruct((MAX_TOKEN,),  jnp.bool_))

                def _host_call(*flat_np):
                    # flat_np æ˜¯å›ä¼ çš„æ‰å¹³åˆ—è¡¨/å…ƒç»„ï¼Œéœ€ç”¨ treedef.unflatten è¿˜åŸ
                    np_dict = treedef.unflatten(list(flat_np))
                    return self._torch_forward(
                        self.inner, np_dict, self.device, self.patch_size
                    )

                feat, valid_mask = pure_callback(
                    _host_call, out_struct, *flat, vectorized=False
                )

                # ---- Runtime contract check: channel dim must match projector ----
                assert feat.shape[-1] == self.enc_out_dim, (
                    f"[SonataWrapper] Output dim {feat.shape[-1]} "
                    f"!= expected {self.enc_out_dim}. "
                    "Check Sonata ckpt / config."
                )
                # ç›´æ¥è¿”å› (feat, valid_mask)ï¼›ç”±è°ƒç”¨æ–¹è‡ªè¡Œæ„é€  mask
                return feat, valid_mask

            # ---------- NNX åˆå§‹åŒ– ----------
            def init_with_output(
                self,
                rngs,
                pc_dict,
                *,
                train: bool = False,
                method: typing.Any = None,
                **_,
            ):
                dummy_out = self.forward(pc_dict, train=train)
                return dummy_out, {}  # æœ¬ wrapper ä¸å«å¯è®­ç»ƒå‚æ•°

            # --------------------------------------------------------------
            # â˜… å…¼å®¹ nnxâ€‘bridge è°ƒç”¨ï¼šé‡è½½ applyï¼Œå¿½ç•¥ variables / rngs. ä»¥åéœ€è¦rngéœ€è¦å°†å…¶ä¼ å…¥ä½†ç›®å‰ä¼ å…¥ä¼šé€ æˆå…¼å®¹æ€§é—®é¢˜
            # --------------------------------------------------------------
            def apply(                       # type: ignore[override]
                self,
                _variables,                  # nnxâ€‘bridge ä¼ å…¥çš„å ä½å˜é‡åŒ…ï¼ˆunusedï¼‰
                *args,
                rngs=None,
                method: str | None = "forward",
                **kwargs,
            ):
                """
                â€¢ method ä¸º nnxâ€‘bridge æŒ‡å®šçš„å‡½æ•°åï¼Œä¾‹å¦‚ "forward"ã€
                  "init_with_output"ï¼›é»˜è®¤ = "forward"  
                â€¢ rngs ä»…åœ¨ lazy_init æ—¶ä¼šä¼ å…¥ï¼ŒSonata ä¸ä½¿ç”¨ï¼Œç›´æ¥ä¸¢å¼ƒ
                """
                if method is None:
                    method = "forward"
    
                # é€‰å®šè¢«è°ƒå‡½æ•°
                target_fn = getattr(self, method)
    
                # init_with_output çš„ç­¾åä¸º (rngs, pc_dict, â€¦)
                if method == "init_with_output":
                    return target_fn(rngs, *args, **kwargs)
    
                # å…¶ä½™æ–¹æ³•ï¼ˆforward ç­‰ï¼‰
                return target_fn(*args, **kwargs)
                
        # ------------------------------------------------------------------
        # 4â€‘bis) çº¿æ€§å±‚åŒ…è£…å™¨ï¼šè®©æ™®é€š Linear æ”¯æŒ lazy_init
        # ------------------------------------------------------------------
        class _TorchLinearWrapper(torch.nn.Module):
            def __init__(self, in_dim: int, out_dim: int, device: torch.device, *, bias: bool = True):
                super().__init__()
                self.inner = torch.nn.Linear(in_dim, out_dim, bias=bias).to(device)
                self.device = device

            # -------- hostâ€‘sideè®¡ç®— --------
            @staticmethod
            @torch.no_grad()
            def _torch_forward(inner: torch.nn.Linear,
                               mat_np: np.ndarray,
                               device: torch.device) -> np.ndarray:
                """
               çº¯ host è°ƒç”¨ï¼šnp -> torch(cuda) -> np
                """
                # ä¿è¯ä¸€å®šæ˜¯ NumPyï¼Œå†é€å…¥ PyTorch
                if not isinstance(mat_np, np.ndarray):
                    mat_np = np.asarray(mat_np)
                x = torch.from_numpy(mat_np).to(inner.weight.dtype).to(device)
                y = inner(x)
                return y.cpu().numpy()

            # -------- forwardï¼ˆJAX sideï¼‰--------
            def forward(self, x_jax: jax.Array, *, train: bool = False):
                """
                â€¢ åœ¨ jit å›¾é‡Œä»¥ pure_callback æ–¹å¼è°ƒç”¨ _torch_forward  
                â€¢ è¾“å‡ºä¿æŒ float32ï¼Œåç»­å† cast
                """
                # è®¡ç®—è¾“å‡ºå½¢çŠ¶ï¼ˆå®Œå…¨ç¬¦å·åŒ–ï¼Œä¸è§¦å‘ concreteï¼‰
                out_shape = (*x_jax.shape[:-1], self.inner.out_features)
                out_struct = ShapeDtypeStruct(out_shape, jnp.float32)

                def _host_call(mat):
                    # mat æ˜¯ numpy.ndarray ï¼ˆpure_callback å·²è½¬æ¢ï¼‰
                    return self._torch_forward(self.inner, mat, self.device)

                return pure_callback(_host_call, out_struct, x_jax, vectorized=False)

            # -------- lazy_init hook --------
            def init_with_output(self, rngs, x_np, *, method=None, **_):
                """
                lazy_initâ€¯é˜¶æ®µä¸ä¼šåœ¨ jit å†…ï¼Œ
                ç›´æ¥èµ° hostâ€‘side è®¡ç®—å³å¯ï¼ŒåŠ é€Ÿåˆå§‹åŒ–ã€‚
                """
                if isinstance(x_np, jax.Array):
                    x_np = np.asarray(jax.device_get(x_np))
                y = self._torch_forward(self.inner, x_np, self.device)
                return y, {}          # no trainable vars

            # -------- nnxâ€‘bridge å…¼å®¹ --------
            def apply(self, _vars, *args, rngs=None, method: str | None = "forward", **kw):
                if method is None:
                    method = "forward"
                fn = getattr(self, method)
                if method == "init_with_output":
                    return fn(rngs, *args, **kw)
                return fn(*args, **kw)


        # ---------- 5) wrap ä¸º NNX æ¨¡å—å¹¶ lazy_init ----------
        # â‘¤ çº¿æ€§æŠ•å½±ï¼šenc_out_dim â†’ PaLIâ€‘Gemma hidden_size
        self.PointProjector = nnx_bridge.ToNNX(
            _TorchLinearWrapper(self._enc_out_dim, _model_width, self.device)
        )
        # small dummy â†’ lazy_init
        self.PointProjector.lazy_init(
            jnp.zeros((1, self._enc_out_dim), jnp.float32),
            rngs=rngs,
        )
        # è®©æŠ•å½±å±‚è·Ÿéš PyTorch çš„ deviceï¼Œä¸å¿…æ‰‹åŠ¨è¿ç§»é™¤éåç»­ç»§ç»­é‡å†™åˆ°jax

        N, B = 64, 1          # 64 points, 1 batch
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Sonata çº¦å®šçš„ Point ç»“æ„ï¼ˆåŠ¡å¿…æ‰å¹³åŒ–ï¼‰ï¼š
        #   coord : (Total_N, 3)   float32 / int32
        #   feat  : (Total_N, C)
        #   batch : (Total_N,)     **int64**  â† è¦èƒ½åš Â«batchÂ <<Â 48Â»
        #   offset: (B,)           ç´¯è®¡ç‚¹æ•°å‰ç¼€å’Œ  **int64**
        #   grid_size : (B, 3)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        raw_dummy_pc = {
            "coord": jnp.arange(N * 3, dtype=jnp.float32).reshape(N, 3),
            "feat":  jnp.zeros((N, self._point_in_channels), dtype=jnp.float32),
            "batch": jnp.zeros((N,),  dtype=jnp.int64),      # 1â€‘D  (Sonata éœ€ int64)
            "offset": jnp.array([N], dtype=jnp.int64),
            "grid_size": jnp.array([[32, 32, 32]], dtype=jnp.int32),   # <= num_bins / 2**4
        }
        dummy_pc = _canonicalize_point_dict(raw_dummy_pc)

        _patch_sz = sp_cfg["enc_patch_size"][-1]   # 1024 (ä¿æŒä¸ ckpt ä¸€è‡´)
        point = nnx_bridge.ToNNX(
            _TorchSonataWrapper(
                point_model,
                self.device,
                _patch_sz,
                self._enc_out_dim,   # â† ä¸ __init__ å¯¹é½
            )
        )
        
        # åˆå§‹åŒ– wrapperï¼ˆä¸€æ¬¡æ€§ shape æ¨æ–­ï¼‰
        point.lazy_init(dummy_pc, train=False, rngs=rngs)

        # ------------------------------------------------------------------
        # 6) æ‰“åŒ…æ‰€æœ‰å­æ¨¡å—
        # ------------------------------------------------------------------
        self.PaliGemma = nnx.Dict(
            llm   = llm,
            img   = img,
            point = point,
            point_proj = self.PointProjector,
        )

    def embed_inputs(
        self, obs: _model.Observation
    ) -> tuple[jax.Array, jax.Array, jax.Array]:
        """
        Embed all input modalities (images, point cloud, text tokens) into sequences of token embeddings.
        Returns:
            token_embeddings (jax.Array): concatenated token features of shape [B, N, emb_dim]
            input_mask (jax.Array[bool]): mask indicating valid tokens [B, N]
            ar_mask (jax.Array[int]): autoregressive mask for tokens [B, N] (0 where tokens attend freely, 1 where causal dependence begins).
        """
        token_embeddings = []
        input_mask = []
        ar_mask = []

        # 1. Image tokens (from image encoder)
        for cam_name, image in obs.images.items():
            img_tokens, _ = self.PaliGemma.img(image, train=False)  # shape [B, n_img_tokens, emb_dim]
            token_embeddings.append(img_tokens)
            # For each image token, the mask is valid if the image is present (broadcast image mask per token)
            mask = jnp.broadcast_to(obs.image_masks[cam_name][:, None], (obs.image_masks[cam_name].shape[0], img_tokens.shape[1]))
            input_mask.append(mask)
            # Image tokens have no autoregressive dependency amongst themselves (set AR mask = 0 for these tokens)
            ar_mask.append(jnp.zeros_like(mask, dtype=jnp.int32))

        # 2. Point cloud tokens --------------------------------------------------
        pc_pack = _extract_point_batch(obs)
        if pc_pack is not None:
            # --------------------------------------------------------
            # ä¸ SpatialLMâ€‘Qwen forward_point_cloud å®Œå…¨ä¸€è‡´çš„ç­–ç•¥ï¼š
            # forâ€‘loop é€æ ·æœ¬è°ƒç”¨ Sonata â†’ æ¯æ¬¡å¾—åˆ° (K*1024, C)ï¼Œ
            # å†ç»Ÿä¸€ pad åˆ°åŒä¸€é•¿åº¦ã€‚
            # --------------------------------------------------------
            pc_dict_all, pc_frame_mask = pc_pack        # pc_frame_mask : [B, M]
            B = pc_dict_all["offset"].shape[0]

            # è¿è¡Œæ—¶ç»´åº¦æ£€æŸ¥
            if pc_dict_all["feat"].shape[-1] != self._point_in_channels:
                raise ValueError(
                    f"Sonata in_channels={self._point_in_channels}, "
                    f"ä½†è¾“å…¥ç‰¹å¾ç»´åº¦={pc_dict_all['feat'].shape[-1]}"
                )

            per_sample_tokens  = []
            per_sample_masks   = []
            max_len            = 0

            for b in range(B):                                    # **é€ batch**
                # ä»…æŠŠ sample id ä¼ ç»™ wrapperï¼›çœŸæ­£çš„åˆ‡ç‰‡åœ¨ PyTorch ä¾§å®Œæˆï¼Œ
                # å› æ­¤è¿™é‡Œä¸å†äº§ç”ŸåŠ¨æ€å½¢çŠ¶ã€‚
                single_dict = {
                    **pc_dict_all,          # å…¨é‡ç‚¹äº‘
                    "selected_batch": jnp.array(b, jnp.int32),  # æ–°å¢é”®
                }
                # grid_coord ç¼ºå¤±åˆ™ç›´æ¥æˆªæ–­å– int â€”â€” ä¸ SpatialLM ç›¸åŒ
                if "grid_coord" not in single_dict:
                    single_dict["grid_coord"] = single_dict["coord"].astype(jnp.int32)

                tok, vmask = self.PaliGemma.point(single_dict, train=False)

                # -------- é•¿åº¦ä¸€è‡´æ€§æ–­è¨€ï¼ˆé—®é¢˜â€¯4ï¼‰--------
                assert tok.shape[0] == vmask.shape[0], (
                    f"Sonata è¿”å› token é•¿åº¦ {tok.shape[0]} "
                    f"â‰  valid_mask é•¿åº¦ {vmask.shape[0]}"
                )

                # æŠ•å½±åˆ° LLM hidden_size ï¼Œä¿æŒä¸å›¾åƒ / æ–‡æœ¬ç»´åº¦ä¸€è‡´
                tok = self.PointProjector(tok.astype(jnp.float32))
                tgt_dtype = token_embeddings[0].dtype if token_embeddings else tok.dtype
                tok = tok.astype(tgt_dtype)

                # SpatialLM ä¿ç•™è¡¥é›¶ tokenï¼Œé  vmask æŒ‡ç¤ºæœ‰æ•ˆæ€§
                per_sample_tokens.append(tok)       # â† ç›´æ¥æ•´å—ä¿å­˜
                per_sample_masks.append(vmask)
                max_len = max(max_len, tok.shape[0])   # ä¸€èˆ¬å°±æ˜¯ 1024

            # ----- pad åˆ° batch å†…æœ€å¤§é•¿åº¦ -----------
            def _pad_to(x, tgt):
                pad = [(0, tgt - x.shape[0])] + [(0, 0)]*(x.ndim-1)
                return jnp.pad(x, pad)

            pt_tokens = jnp.stack([_pad_to(t, max_len) for t in per_sample_tokens])  # (B,max_len,C)
            valid_m   = jnp.stack([_pad_to(m, max_len) for m in per_sample_masks])

            # äºŒæ¬¡éªŒè¯ï¼šæ‰€æœ‰ batch é•¿åº¦å·²ç»å¯¹é½
            assert (pt_tokens.shape[1] == valid_m.shape[1] == max_len), \
                "pad å token / mask é•¿åº¦ä¸ä¸€è‡´"

            # --- mask å¯¹é½åˆ°å¤–å±‚ pc_frame_mask ---  (ä¸ SpatialLM æ€è·¯ç›¸åŒ)
            if max_len > pc_frame_mask.shape[1]:
                pad_len = max_len - pc_frame_mask.shape[1]
                pc_frame_mask = jnp.pad(pc_frame_mask,
                                         ((0,0),(0,pad_len)),
                                         constant_values=False)
            else:                       # ç½•è§ï¼š1024 < Mï¼Œè£å‰ªå¤–å±‚æ©ç 
                pc_frame_mask = pc_frame_mask[:, :max_len]
            pt_final_mask = pc_frame_mask & valid_m

            token_embeddings.append(pt_tokens)
            input_mask.append(pt_final_mask)
            ar_mask.append(jnp.zeros_like(pt_final_mask, dtype=jnp.int32))

        # 3. Text tokens (from language model embedding)
        # Ensure textual inputs are present
        assert obs.tokenized_prompt is not None and obs.tokenized_prompt_mask is not None and obs.token_ar_mask is not None, \
            "Tokenized prompt and corresponding masks must be provided for text inputs."
        txt_tokens = self.PaliGemma.llm(obs.tokenized_prompt, embed_only=True)  # shape [B, L, emb_dim]
        token_embeddings.append(txt_tokens)
        input_mask.append(obs.tokenized_prompt_mask)
        ar_mask.append(obs.token_ar_mask)

        # Concatenate all modality token sequences along the sequence dimension
        tokens = jnp.concatenate(token_embeddings, axis=1)      # [B, N_total, emb_dim]
        mask = jnp.concatenate(input_mask, axis=1)              # [B, N_total]
        ar = jnp.concatenate(ar_mask, axis=1)                   # [B, N_total]
        return tokens, mask, ar

    def compute_loss(
        self,
        rng: jax.Array,
        observation: _model.Observation,
        actions: _model.Actions,
        *,
        train: bool = False
    ) -> jax.Array:
        """Compute sequence loss for the model (predict next token loss for language prompt)."""
        # Preprocess observation (normalize/augment images, ensure masks)
        # å‘å›¾åƒé¢„å¤„ç†å‡½æ•°ä¼ é€’ RNGï¼ˆä»…åœ¨è®­ç»ƒé˜¶æ®µéœ€è¦ï¼‰
        prep_rng = rng if train else None
        observation = _model.preprocess_observation(prep_rng, observation, train=train, image_keys=list(observation.images.keys()))
        # Embed all inputs to tokens and get masks
        tokens, mask, ar = self.embed_inputs(observation)
        # Compute attention mask for the sequence (prefix + causal masking as needed)
        attn_mask = _pi0_fast.make_attn_mask(mask, ar)
        # Prepare next-token prediction targets: one-hot encoding of tokenized_prompt shifted by one
        vocab_size = self.PaliGemma.llm.module.vocab_size
        targets = jax.nn.one_hot(observation.tokenized_prompt[:, 1:], vocab_size)  # shape [B, L-1, vocab_size]
        # Run LLM for prefix (all tokens except the last one, which will be predicted)
        pre_logits, _, _ = self.PaliGemma.llm(
            embedded_prefix=tokens[:, :-1],
            mask=attn_mask[:, :-1, :-1],
            return_prelogits=True
        )
        # Decode only the new suffix (the part to predict) to get logits for target tokens
        logits, _ = self.PaliGemma.llm(pre_logits=pre_logits[:, -targets.shape[1]:])
        log_probs = jax.nn.log_softmax(logits, axis=-1)
        # Compute cross-entropy loss on the target token predictions
        assert observation.token_loss_mask is not None, "Token loss mask must be provided for computing loss."
        loss_mask = observation.token_loss_mask[:, 1:]  # align with targets (skip the first token which has no previous token to predict it)
        token_nll = -jnp.sum(targets * log_probs, axis=-1)  # negative log-likelihood per token
        # Compute mean loss per sequence (only averaging over actual target tokens)
        seq_loss = jnp.sum(token_nll * loss_mask, axis=-1) / jnp.clip(jnp.sum(loss_mask, axis=-1), a_min=1)
        return seq_loss  # shape [B], per-sequence loss

    def sample_actions(
        self,
        rng: jax.Array,
        observation: _model.Observation,
        *,
        max_decoding_steps: int = 256,
        temperature: float = 0.0
    ) -> _model.Actions:
        """
        Autoregressively sample a sequence of actions (or action tokens) from the model given an observation.
        This uses the model as a prefix model (prefix = images + prompt tokens + optional point tokens),
        then generates additional tokens up to max_decoding_steps or until an EOS token is produced.
        """
        # Preprocess observation (no augmentation, just ensure correct shapes/masks)
        observation = _model.preprocess_observation(None, observation, train=False, image_keys=list(observation.images.keys()))
        # Embed inputs to get prefix token embeddings and masks
        prefix_tokens, prefix_mask, prefix_ar_mask = self.embed_inputs(observation)
        prefix_attn_mask = _pi0_fast.make_attn_mask(prefix_mask, prefix_ar_mask)
        # Align all prefix sequences to the right (required for caching in prefix)
        prefix_tokens, prefix_mask, prefix_attn_mask = _pi0_fast.left_to_right_align(prefix_tokens, prefix_mask, prefix_attn_mask)
        # â‘   â€”â€”  ä¸ SpatialLM ä¸€è‡´ï¼šprefill_len æ˜ç¡® cast ä¸º int32ï¼ˆåç»­ä¸ lax.iota ç­‰ä¿æŒç±»å‹åŒ¹é…ï¼‰
        prefill_size = prefix_tokens.shape[1]   # total sequence length after alignment (prefix padded to some length)
        prefill_len = jnp.sum(prefix_mask, axis=-1).astype(jnp.int32)   # actual prefix length per batch (number of valid tokens)
        prefix_start = prefill_size - prefill_len   # start index of actual prefix tokens after right-align (for each batch)
        # Prepare attention mask for prefix + decoding steps
        prefix_attn_mask = jnp.pad(prefix_attn_mask, ((0, 0), (0, 0), (0, max_decoding_steps)))
        prefix_positions = jnp.cumsum(prefix_mask, axis=-1) - 1  # positions of prefix tokens (0-indexed)
        # Run the LLM in decoding mode to fill KV cache with prefix
        prefix_logits, kv_cache, _ = self.PaliGemma.llm(
            embedded_prefix=prefix_tokens,
            mask=prefix_attn_mask,
            positions=prefix_positions,
            decode=True
        )
        # Start from the last logit of the prefix as the beginning for new generation
        last_logit = prefix_logits[:, -1:]   # [B, 1, V]
        # Placeholder for generated token outputs (initialize with zeros)
        output_tokens = jnp.zeros((last_logit.shape[0], max_decoding_steps), dtype=jnp.int32)

        # Now run autoregressive decoding for at most max_decoding_steps
        tokens_to_decode = max_decoding_steps
        # Prepare attention mask for single-step decoding (prefix length + current step)
        # We will update the attention mask dynamically in the loop if needed

        # Use a while loop to generate tokens until done or max steps
        def cond_fn(state):
            _rng, _last, _cache, _step, _out = state
            has_eos = jax.lax.cond(
                _step == 0,
                lambda _: False,
                lambda _: jnp.all(_out[:, _step - 1] == _pi0_fast.PALIGEMMA_EOS_TOKEN),
                operand=None,
            )
            return jnp.logical_and(_step < max_decoding_steps, ~has_eos)

        def body_fn(state):
            rng_key, last_logits, cache, step, out_tokens = state
            rng_key, subkey = jax.random.split(rng_key)
            logits_step = last_logits.squeeze(1)
            token = jax.lax.cond(
                temperature > 1e-6,
                lambda key: jax.random.categorical(key, logits_step / temperature, axis=-1),
                lambda key: jnp.argmax(logits_step, axis=-1),
                operand=subkey,
            ).astype(jnp.int32)
            # åŸ put_along_last_axis æ„é€  O(NÂ²) oneâ€‘hotï¼›æ”¹ç”¨ scatter æ›´æ–°
            out_tokens = out_tokens.at[:, step].set(token)

            # Gemmaâ€‘fast & SpatialLMï¼šä½ç½® = å·²å¡« prefix token æ•° + å½“å‰ step
            # â‘¡  â€”â€”  positions ç»Ÿä¸€ int32ï¼Œå¯é¿å… multiâ€‘host sharding â€œmixed signednessâ€ æŠ¥è­¦
            positions = (prefill_len[:, None] + step).astype(jnp.int32)
            # Gemmaâ€‘fast æ—  token=kwargï¼šå…ˆåµŒå…¥ï¼Œå† decode ä¸€æ­¥
            token_emb = self.PaliGemma.llm(token[:, None], embed_only=True)
            logits, cache = self.PaliGemma.llm(
                embedded_prefix=token_emb,
                kv_cache=cache,
                positions=positions,
                decode=True,
            )
            return rng_key, logits, cache, step+1, out_tokens

        init_state = (rng, last_logit, kv_cache, jnp.array(0, jnp.int32), output_tokens)
        _, _, _, final_step, output_seq = jax.lax.while_loop(cond_fn, body_fn, init_state)

        # Return the output sequence of tokens as the model's predicted "actions"
        # (In practice, these tokens might represent discretized actions or a planned sequence encoded as text tokens)
        return output_seq[:, :final_step]   # [B, <=max_dec_steps]